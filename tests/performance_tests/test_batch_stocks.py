#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹æ¬¡è‚¡ç¥¨æ¸¬è©¦ - æ¸¬è©¦å¤šæª”è‚¡ç¥¨å’Œ TPEx åŠŸèƒ½
"""

import sys
import os
sys.path.insert(0, 'src/data')

from price_data_pipeline import TaiwanStockPriceDataPipeline
import sqlite3
import pandas as pd
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_batch_stocks():
    """æ¸¬è©¦æ‰¹æ¬¡è‚¡ç¥¨æŠ“å–"""
    
    # æ¸¬è©¦è‚¡ç¥¨æ¸…å–® (æ··åˆä¸Šå¸‚/ä¸Šæ«ƒ)
    test_stocks = [
        ("2330", "å°ç©é›»", "TWSE"),
        ("2454", "è¯ç™¼ç§‘", "TWSE"), 
        ("1101", "å°æ³¥", "TWSE"),
        ("6488", "ç’°çƒæ™¶", "TPEx"),  # ä¸Šæ«ƒæ¸¬è©¦
        ("3034", "è¯è© ", "TWSE"),
    ]
    
    logger.info(f"ğŸ§ª æ‰¹æ¬¡æ¸¬è©¦ {len(test_stocks)} æª”è‚¡ç¥¨")
    
    pipeline = TaiwanStockPriceDataPipeline()
    results = []
    
    for stock_id, name, market in test_stocks:
        logger.info(f"\nğŸ“Š æ¸¬è©¦ {stock_id} ({name}) - {market}")
        
        try:
            success = pipeline.fetch_stock_historical_data(stock_id, market, 40)
            
            if success:
                # æŸ¥è©¢çµæœ
                conn = sqlite3.connect(pipeline.db_path)
                df = pd.read_sql_query("""
                    SELECT COUNT(*) as count, MIN(date) as min_date, MAX(date) as max_date, 
                           source, market
                    FROM daily_prices 
                    WHERE stock_id = ?
                    GROUP BY source, market
                """, conn, params=(stock_id,))
                conn.close()
                
                if not df.empty:
                    total_records = df['count'].sum()
                    logger.info(f"âœ… {stock_id}: {total_records} ç­† ({df.iloc[0]['min_date']} ~ {df.iloc[0]['max_date']})")
                    logger.info(f"   ä¾†æº: {df.iloc[0]['source']}, å¸‚å ´: {df.iloc[0]['market']}")
                    
                    results.append({
                        'stock_id': stock_id,
                        'name': name,
                        'market': market,
                        'records': total_records,
                        'success': True,
                        'source': df.iloc[0]['source']
                    })
                else:
                    logger.warning(f"âŒ {stock_id}: ç„¡æ•¸æ“š")
                    results.append({
                        'stock_id': stock_id, 'name': name, 'market': market,
                        'records': 0, 'success': False, 'source': None
                    })
            else:
                logger.error(f"âŒ {stock_id}: æŠ“å–å¤±æ•—")
                results.append({
                    'stock_id': stock_id, 'name': name, 'market': market, 
                    'records': 0, 'success': False, 'source': None
                })
                
        except Exception as e:
            logger.error(f"âŒ {stock_id}: ç•°å¸¸ - {e}")
            results.append({
                'stock_id': stock_id, 'name': name, 'market': market,
                'records': 0, 'success': False, 'source': None
            })
    
    # æ¸¬è©¦çµæœç¸½çµ
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š æ‰¹æ¬¡æ¸¬è©¦çµæœç¸½çµ")
    logger.info("="*60)
    
    success_count = sum(1 for r in results if r['success'])
    total_records = sum(r['records'] for r in results)
    
    logger.info(f"âœ… æˆåŠŸç‡: {success_count}/{len(results)} ({success_count/len(results)*100:.1f}%)")
    logger.info(f"ğŸ“Š ç¸½æ•¸æ“š: {total_records} ç­†è¨˜éŒ„")
    
    # æŒ‰å¸‚å ´åˆ†çµ„
    twse_results = [r for r in results if r['market'] == 'TWSE']
    tpex_results = [r for r in results if r['market'] == 'TPEx']
    
    logger.info(f"ğŸ¢ TWSE: {sum(1 for r in twse_results if r['success'])}/{len(twse_results)} æˆåŠŸ")
    logger.info(f"ğŸª TPEx: {sum(1 for r in tpex_results if r['success'])}/{len(tpex_results)} æˆåŠŸ")
    
    # è©³ç´°çµæœ
    for r in results:
        status = "âœ…" if r['success'] else "âŒ"
        logger.info(f"{status} {r['stock_id']} ({r['name']}) - {r['market']}: {r['records']} ç­†, ä¾†æº: {r['source']}")

def test_freshness_check():
    """æ¸¬è©¦æ–°é®®åº¦æª¢æŸ¥åŠŸèƒ½"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ” æ¸¬è©¦æ–°é®®åº¦æª¢æŸ¥åŠŸèƒ½")
    logger.info("="*60)
    
    pipeline = TaiwanStockPriceDataPipeline()
    
    # æ¸¬è©¦å·²å­˜åœ¨çš„è‚¡ç¥¨ (2330æ‡‰è©²å·²ç¶“æŠ“å–é)
    is_fresh = pipeline.is_fresh_enough("2330", 60, 7)
    logger.info(f"ğŸ’¾ 2330 æ–°é®®åº¦æª¢æŸ¥: {'å¤ æ–°' if is_fresh else 'éœ€æ›´æ–°'}")
    
    # æ¸¬è©¦ä¸å­˜åœ¨çš„è‚¡ç¥¨
    is_fresh = pipeline.is_fresh_enough("9999", 60, 7)
    logger.info(f"ğŸ’¾ 9999 æ–°é®®åº¦æª¢æŸ¥: {'å¤ æ–°' if is_fresh else 'éœ€æ›´æ–°'}")

def test_database_summary():
    """æ¸¬è©¦è³‡æ–™åº«çµ±è¨ˆç¸½çµ"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ’¾ è³‡æ–™åº«çµ±è¨ˆç¸½çµ")
    logger.info("="*60)
    
    pipeline = TaiwanStockPriceDataPipeline()
    conn = sqlite3.connect(pipeline.db_path)
    
    # ç¸½é«”çµ±è¨ˆ
    stats = conn.execute("""
        SELECT 
            COUNT(DISTINCT stock_id) as unique_stocks,
            COUNT(*) as total_records,
            MIN(date) as earliest_date,
            MAX(date) as latest_date,
            COUNT(DISTINCT market) as markets,
            COUNT(DISTINCT source) as sources
        FROM daily_prices
    """).fetchone()
    
    logger.info(f"ğŸ“Š ç¸½è¦½: {stats[0]} æª”è‚¡ç¥¨, {stats[1]} ç­†è¨˜éŒ„")
    logger.info(f"ğŸ“… æ™‚é–“è·¨åº¦: {stats[2]} ~ {stats[3]}")
    logger.info(f"ğŸ¢ å¸‚å ´æ•¸: {stats[4]}, ä¾†æºæ•¸: {stats[5]}")
    
    # æŒ‰å¸‚å ´çµ±è¨ˆ
    market_stats = conn.execute("""
        SELECT market, COUNT(DISTINCT stock_id) as stocks, COUNT(*) as records
        FROM daily_prices
        GROUP BY market
        ORDER BY records DESC
    """).fetchall()
    
    for market, stocks, records in market_stats:
        logger.info(f"ğŸ“ˆ {market}: {stocks} æª”è‚¡ç¥¨, {records} ç­†è¨˜éŒ„")
    
    # æŒ‰ä¾†æºçµ±è¨ˆ
    source_stats = conn.execute("""
        SELECT source, COUNT(*) as records
        FROM daily_prices
        GROUP BY source
        ORDER BY records DESC
    """).fetchall()
    
    for source, records in source_stats:
        logger.info(f"ğŸ”— {source}: {records} ç­†è¨˜éŒ„")
    
    conn.close()

if __name__ == "__main__":
    test_batch_stocks()
    test_freshness_check()
    test_database_summary()