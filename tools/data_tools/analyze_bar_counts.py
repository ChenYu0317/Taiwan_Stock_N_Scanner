#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æžç‚ºä»€éº¼æœƒæœ‰ä¸åŒçš„Kç·šæ ¹æ•¸
"""

import sys
import os
sys.path.insert(0, 'src/data')

from price_data_pipeline import TaiwanStockPriceDataPipeline
import sqlite3
import pandas as pd
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def analyze_bar_counts():
    """åˆ†æžä¸åŒè‚¡ç¥¨çš„Kç·šæ ¹æ•¸å·®ç•°"""
    
    pipeline = TaiwanStockPriceDataPipeline()
    conn = sqlite3.connect(pipeline.db_path)
    
    # æŸ¥è©¢æ¯æª”è‚¡ç¥¨çš„è©³ç´°ä¿¡æ¯
    query = """
    SELECT 
        stock_id,
        COUNT(*) as total_bars,
        MIN(date) as earliest_date,
        MAX(date) as latest_date,
        source,
        market
    FROM daily_prices 
    GROUP BY stock_id, source, market
    ORDER BY stock_id
    """
    
    df = pd.read_sql_query(query, conn)
    conn.close()
    
    logger.info("ðŸ“Š è‚¡ç¥¨Kç·šæ ¹æ•¸åˆ†æž")
    logger.info("=" * 60)
    
    for _, row in df.iterrows():
        logger.info(f"ðŸ“ˆ {row['stock_id']} ({row['market']})")
        logger.info(f"   æ ¹æ•¸: {row['total_bars']} ç­†")
        logger.info(f"   æœŸé–“: {row['earliest_date']} ~ {row['latest_date']}")
        logger.info(f"   ä¾†æº: {row['source']}")
        
        # è¨ˆç®—äº¤æ˜“æ—¥æ•¸é‡
        start_date = pd.to_datetime(row['earliest_date'])
        end_date = pd.to_datetime(row['latest_date'])
        total_days = (end_date - start_date).days + 1
        
        logger.info(f"   ç¸½å¤©æ•¸: {total_days} å¤©")
        logger.info(f"   äº¤æ˜“æ—¥æ¯”çŽ‡: {row['total_bars']}/{total_days} = {row['total_bars']/total_days:.2%}")
        logger.info("")

def analyze_fetch_logic():
    """åˆ†æžæŠ“å–é‚è¼¯"""
    
    logger.info("ðŸ” åˆ†æžæŠ“å–é‚è¼¯å·®ç•°")
    logger.info("=" * 60)
    
    # æˆ‘å€‘çš„æ¸¬è©¦ç”¨ä¸åŒçš„ target_bars åƒæ•¸
    test_cases = [
        ("2330", "TWSE", 60),  # å°ç©é›»è¦æ±‚60æ ¹
        ("2454", "TWSE", 40),  # è¯ç™¼ç§‘è¦æ±‚40æ ¹  
        ("1101", "TWSE", 40),  # å°æ³¥è¦æ±‚40æ ¹
        ("6488", "TPEx", 40),  # ç’°çƒæ™¶è¦æ±‚40æ ¹
        ("3034", "TWSE", 40),  # è¯è© è¦æ±‚40æ ¹
    ]
    
    logger.info("æ¸¬è©¦åƒæ•¸è¨­å®š:")
    for stock_id, market, target in test_cases:
        logger.info(f"  {stock_id} ({market}): ç›®æ¨™ {target} æ ¹")
    
    logger.info("\nä½†æ˜¯ç‚ºä»€éº¼å¯¦éš›çµæžœä¸åŒï¼Ÿ")
    
    # æª¢æŸ¥æ–°é®®åº¦æª¢æŸ¥çš„å½±éŸ¿
    pipeline = TaiwanStockPriceDataPipeline()
    
    for stock_id, market, target in test_cases:
        is_fresh = pipeline.is_fresh_enough(stock_id, target, 7)
        logger.info(f"  {stock_id}: æ–°é®®åº¦æª¢æŸ¥ = {is_fresh}")
        
        if is_fresh:
            logger.info(f"    â†’ è·³éŽæŠ“å–ï¼Œä½¿ç”¨ç¾æœ‰æ•¸æ“š")
        else:
            logger.info(f"    â†’ éœ€è¦æŠ“å– {target} æ ¹")

def check_actual_test_calls():
    """æª¢æŸ¥å¯¦éš›çš„æ¸¬è©¦èª¿ç”¨"""
    
    logger.info("\nðŸ” æª¢æŸ¥æ¸¬è©¦è…³æœ¬çš„å¯¦éš›èª¿ç”¨")
    logger.info("=" * 60)
    
    # æª¢æŸ¥ test_batch_stocks.py ä¸­çš„å¯¦éš›èª¿ç”¨
    try:
        with open('test_batch_stocks.py', 'r', encoding='utf-8') as f:
            content = f.read()
            
        # å°‹æ‰¾ fetch_stock_historical_data èª¿ç”¨
        lines = content.split('\n')
        for i, line in enumerate(lines):
            if 'fetch_stock_historical_data' in line:
                logger.info(f"ç¬¬ {i+1} è¡Œ: {line.strip()}")
                
        # å°‹æ‰¾ target_bars è¨­å®š
        for i, line in enumerate(lines):
            if 'target_bars' in line or '60' in line or '40' in line:
                if 'fetch_stock_historical_data' in line:
                    logger.info(f"èª¿ç”¨è¡Œ {i+1}: {line.strip()}")
                    
    except Exception as e:
        logger.error(f"ç„¡æ³•è®€å–æ¸¬è©¦è…³æœ¬: {e}")

def check_database_history():
    """æª¢æŸ¥æ•¸æ“šåº«æ­·å²"""
    
    logger.info("\nðŸ“š æª¢æŸ¥æ•¸æ“šåº«ç´¯ç©æ­·å²")
    logger.info("=" * 60)
    
    pipeline = TaiwanStockPriceDataPipeline()
    conn = sqlite3.connect(pipeline.db_path)
    
    # æŸ¥è©¢æ¯æª”è‚¡ç¥¨çš„æ‰€æœ‰è¨˜éŒ„ï¼ŒæŒ‰æ™‚é–“æŽ’åº
    stocks = ['2330', '2454', '1101', '6488', '3034']
    
    for stock_id in stocks:
        query = """
        SELECT date, source, ingested_at 
        FROM daily_prices 
        WHERE stock_id = ? 
        ORDER BY date
        """
        
        df = pd.read_sql_query(query, conn, params=(stock_id,))
        
        if len(df) > 0:
            logger.info(f"ðŸ“ˆ {stock_id}: ç¸½å…± {len(df)} ç­†è¨˜éŒ„")
            logger.info(f"   æœ€æ—©: {df.iloc[0]['date']} ({df.iloc[0]['source']})")
            logger.info(f"   æœ€æ–°: {df.iloc[-1]['date']} ({df.iloc[-1]['source']})")
            
            # æª¢æŸ¥æ˜¯å¦æœ‰å¤šæ¬¡æŠ“å–
            sources = df['source'].value_counts()
            logger.info(f"   ä¾†æºåˆ†å¸ƒ: {sources.to_dict()}")
            
            # æª¢æŸ¥æ™‚é–“é–“éš”
            df['date'] = pd.to_datetime(df['date'])
            gaps = df['date'].diff().dt.days
            weekend_gaps = gaps[(gaps > 1) & (gaps <= 3)]  # é€±æœ«
            long_gaps = gaps[gaps > 3]  # é•·æœŸé–“éš”
            
            if len(long_gaps) > 0:
                logger.info(f"   é•·é–“éš”: {len(long_gaps)} è™•ï¼Œæœ€å¤§ {long_gaps.max()} å¤©")
            
        logger.info("")
    
    conn.close()

if __name__ == "__main__":
    analyze_bar_counts()
    analyze_fetch_logic() 
    check_actual_test_calls()
    check_database_history()