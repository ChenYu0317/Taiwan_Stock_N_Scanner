#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°è‚¡æ­·å²åƒ¹æ ¼æ•¸æ“šç²å–ç®¡é“
ä¿®å¾©Phase 1éºæ¼ï¼šå¾å®˜æ–¹ä¾†æºç²å–çœŸå¯¦çš„æ­·å²OHLCæ•¸æ“š
"""

import requests
import pandas as pd
import sqlite3
import time
import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
import os
from urllib.parse import urlencode
import calendar
from dateutil.relativedelta import relativedelta
from dateutil import tz
import math
import numpy as np
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import re
from collections import deque

# å°åŒ—æ™‚å€å®šç¾©
TAIPEI = tz.gettz("Asia/Taipei")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å…¨åŸŸé€Ÿç‡é™åˆ¶å™¨
class RateLimiter:
    def __init__(self, max_requests=6, window_seconds=1.0):
        self.max_requests = max_requests
        self.window_seconds = window_seconds
        self.hits = deque()
    
    def acquire(self):
        """ç²å–è«‹æ±‚è¨±å¯ï¼Œå¦‚éœ€è¦æœƒè‡ªå‹•ç­‰å¾…"""
        now = time.time()
        
        # ç§»é™¤çª—å£å¤–çš„èˆŠè«‹æ±‚
        while self.hits and now - self.hits[0] > self.window_seconds:
            self.hits.popleft()
        
        # å¦‚æœé”åˆ°é™åˆ¶ï¼Œç­‰å¾…åˆ°æœ€èˆŠè«‹æ±‚éæœŸ
        if len(self.hits) >= self.max_requests:
            sleep_time = self.window_seconds - (now - self.hits[0]) + 0.001
            if sleep_time > 0:
                time.sleep(sleep_time)
        
        # è¨˜éŒ„æ–°è«‹æ±‚
        self.hits.append(time.time())

# å…¨åŸŸé€Ÿç‡é™åˆ¶å™¨å¯¦ä¾‹
rate_limiter = RateLimiter(max_requests=6, window_seconds=1.0)

def sanitize_stock_id(stock_id: str) -> str:
    """æ ¡é©—è‚¡ç¥¨ä»£ç¢¼æ ¼å¼"""
    if not re.fullmatch(r"\d{4}", str(stock_id)):
        raise ValueError(f"invalid stock_id: {stock_id}")
    return stock_id

def ensure_daily_prices_table(conn):
    """å»ºç«‹å–®ä¸€åƒ¹æ ¼äº‹å¯¦è¡¨"""
    conn.execute("""
    CREATE TABLE IF NOT EXISTS daily_prices (
        stock_id TEXT NOT NULL,
        date DATE NOT NULL,
        open REAL, high REAL, low REAL, close REAL, volume INTEGER,
        market TEXT, source TEXT,
        ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        PRIMARY KEY (stock_id, date)
    )
    """)
    conn.execute("CREATE INDEX IF NOT EXISTS idx_prices_date ON daily_prices(date)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_prices_market ON daily_prices(market)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_prices_stock_date ON daily_prices(stock_id, date)")
    conn.commit()

class TaiwanStockPriceDataPipeline:
    """å°è‚¡æ­·å²åƒ¹æ ¼æ•¸æ“šç®¡é“"""
    
    def __init__(self, db_path: str = "data/cleaned/taiwan_stocks_cleaned.db"):
        self.db_path = db_path
        self.session = requests.Session()
        
        # å„ªåŒ–é€£ç·šæ± è¨­å®š
        adapter = HTTPAdapter(
            pool_connections=100,  # é€£ç·šæ± å¤§å°
            pool_maxsize=100,      # æ¯å€‹é€£ç·šæ± çš„æœ€å¤§é€£ç·šæ•¸
            max_retries=Retry(
                total=3, 
                backoff_factor=0.2,  # æ›´å¿«çš„é‡è©¦
                status_forcelist=[429, 500, 502, 503, 504]
            )
        )
        
        self.session.mount("https://", adapter)
        self.session.mount("http://", adapter)
        
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        })

    def is_fresh_enough(self, stock_id: str, target_bars: int, freshness_days: int = 7) -> bool:
        """æª¢æŸ¥è‚¡ç¥¨æ•¸æ“šæ˜¯å¦è¶³å¤ æ–°é®®ï¼Œé¿å…é‡è¤‡æŠ“å–"""
        conn = sqlite3.connect(self.db_path)
        try:
            result = conn.execute(
                "SELECT COUNT(*), MAX(date) FROM daily_prices WHERE stock_id=?",
                (stock_id,)
            ).fetchone()
            c, maxd = result or (0, None)
        finally:
            conn.close()
            
        if not maxd:
            return False
            
        maxd = pd.to_datetime(maxd)
        today = pd.Timestamp.now(tz=TAIPEI).normalize().tz_localize(None)
        
        if c >= target_bars and (today - maxd).days <= freshness_days:
            return True
        return False
        
    def get_stock_list(self) -> List[Tuple[str, str, str]]:
        """å¾è³‡æ–™åº«ç²å–è‚¡ç¥¨æ¸…å–®"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT stock_id, name, market FROM stock_universe WHERE status='active' ORDER BY stock_id")
            stocks = cursor.fetchall()
            conn.close()
            return stocks
        except Exception as e:
            logger.error(f"ç²å–è‚¡ç¥¨æ¸…å–®å¤±æ•—: {e}")
            return []
    
    def fetch_twse_stock_data(self, stock_id: str, year: int, month: int) -> Optional[pd.DataFrame]:
        """
        å¾TWSEç²å–å–®ä¸€è‚¡ç¥¨çš„æœˆåº¦æ­·å²è³‡æ–™
        
        Args:
            stock_id: è‚¡ç¥¨ä»£ç¢¼
            year: å¹´ä»½
            month: æœˆä»½
            
        Returns:
            DataFrame æˆ– None
        """
        try:
            # TWSE STOCK_DAY API
            url = "https://www.twse.com.tw/exchangeReport/STOCK_DAY"
            params = {
                'response': 'json',
                'date': f"{year}{month:02d}01",
                'stockNo': stock_id
            }
            
            rate_limiter.acquire()  # å…¨åŸŸé€Ÿç‡é™åˆ¶
            response = self.session.get(url, params=params, timeout=15)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get('stat') != 'OK':
                logger.warning(f"TWSE APIè¿”å›éOKç‹€æ…‹: {stock_id} {year}/{month}")
                return None
                
            if not data.get('data'):
                logger.debug(f"ç„¡æ•¸æ“š: {stock_id} {year}/{month}")
                return None
            
            # è§£ææ•¸æ“š
            df = pd.DataFrame(data['data'])
            if len(df) == 0:
                return None
            
            # TWSE æ¬„ä½æ”¹ç‰ˆéŸŒæ€§ (å¿…è£œé …ç›®3) - ä¿®æ­£ç‰ˆ
            fields = data.get('fields') or []
            if fields:
                logger.debug(f"TWSE API æ¬„ä½: {fields}")
                
                # å»ºç«‹æ¬„åæ˜ å°„ï¼ˆæ”¯æ´ä¸­è‹±æ–‡ï¼‰
                field_mapping = {}
                for i, field in enumerate(fields):
                    if field in ['æ—¥æœŸ', 'Date']:
                        field_mapping['date'] = i
                    elif field in ['æˆäº¤è‚¡æ•¸', 'æˆäº¤é‡', 'Volume']:
                        field_mapping['volume'] = i
                    elif field in ['é–‹ç›¤åƒ¹', 'é–‹ç›¤', 'Open']:
                        field_mapping['open'] = i
                    elif field in ['æœ€é«˜åƒ¹', 'æœ€é«˜', 'High']:
                        field_mapping['high'] = i
                    elif field in ['æœ€ä½åƒ¹', 'æœ€ä½', 'Low']:
                        field_mapping['low'] = i
                    elif field in ['æ”¶ç›¤åƒ¹', 'æ”¶ç›¤', 'Close']:
                        field_mapping['close'] = i
                
                # ç¢ºèªå¿…è¦æ¬„ä½éƒ½å­˜åœ¨
                required_fields = ['date', 'volume', 'open', 'high', 'low', 'close']
                if all(field in field_mapping for field in required_fields):
                    # åªé¸å–éœ€è¦çš„æ¬„ä½ä¸¦é‡æ–°æ’åº
                    selected_columns = [field_mapping[field] for field in required_fields]
                    df = df.iloc[:, selected_columns]
                    df.columns = required_fields
                    logger.debug(f"ä½¿ç”¨æ™ºèƒ½æ¬„ä½æ˜ å°„ï¼Œé¸å–æ¬„ä½: {required_fields}")
                else:
                    logger.warning(f"TWSE API æ¬„ä½çµæ§‹è®Šæ›´ï¼Œå›é€€åˆ°å›ºå®šæ ¼å¼: {fields}")
                    missing = [f for f in required_fields if f not in field_mapping]
                    logger.warning(f"ç¼ºå°‘æ¬„ä½: {missing}")
                    
                    # å›é€€åˆ°å›ºå®šæ¬„ä½é †åºï¼ˆæ ¹æ“šå¯¦éš›APIå›æ‡‰èª¿æ•´ï¼‰
                    df.columns = ['date', 'volume', 'turnover', 'open', 'high', 'low', 'close', 'change', 'transaction']
                    # é¸å–éœ€è¦çš„æ¬„ä½
                    df = df[['date', 'volume', 'open', 'high', 'low', 'close']]
            else:
                # Fallback: ä½¿ç”¨å›ºå®šæ¬„ä½åç¨±
                logger.warning("TWSE API æœªæä¾›æ¬„ä½è³‡è¨Šï¼Œä½¿ç”¨å›ºå®šæ ¼å¼")
                df.columns = ['date', 'volume', 'turnover', 'open', 'high', 'low', 'close', 'change', 'transaction']
                df = df[['date', 'volume', 'open', 'high', 'low', 'close']]
            
            # æ•¸æ“šæ¸…ç† - è™•ç†æ°‘åœ‹å¹´è½‰è¥¿å…ƒå¹´
            def convert_roc_date(date_str):
                """è½‰æ›æ°‘åœ‹å¹´æ—¥æœŸç‚ºè¥¿å…ƒå¹´"""
                parts = date_str.split('/')
                if len(parts) == 3:
                    roc_year = int(parts[0])
                    month = parts[1]
                    day = parts[2]
                    ad_year = roc_year + 1911  # æ°‘åœ‹å¹´è½‰è¥¿å…ƒå¹´
                    return f"{ad_year}-{month.zfill(2)}-{day.zfill(2)}"
                return date_str
            
            df['date'] = df['date'].apply(convert_roc_date)
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
            
            # è½‰æ›æ•¸å­—æ¬„ä½ (æ›´å®‰å…¨çš„æ•¸å€¼æ¸…æ´—ï¼Œä¿ç•™åŸå§‹å€¼)
            numeric_cols = ['volume', 'open', 'high', 'low', 'close']
            for col in numeric_cols:
                if col in df.columns:  # ç¢ºä¿æ¬„ä½å­˜åœ¨
                    # ä¿ç•™åŸå§‹å€¼ç”¨æ–¼è¿½æº¯
                    df[f'{col}_raw'] = df[col].copy()
                    
                    # æ¸…æ´—ä¸¦è½‰æ›
                    s = df[col].astype(str)
                    s = s.str.replace(",", "", regex=False)
                    s = s.replace({"--": None, "â€”": None, "": None, "nan": None})
                    df[col] = pd.to_numeric(s, errors="coerce")
                else:
                    logger.debug(f"æ¬„ä½ {col} ä¸å­˜åœ¨ï¼Œè·³éè™•ç†")
            
            # éæ¿¾ç„¡æ•ˆæ•¸æ“š
            df = df.dropna(subset=['open', 'high', 'low', 'close'])
            df = df[df['close'] > 0]
            
            if len(df) == 0:
                return None
                
            # åªä¿ç•™éœ€è¦çš„æ¬„ä½ä¸¦æ·»åŠ ä¾†æºæ¨™è¨˜
            result = df[['date', 'open', 'high', 'low', 'close', 'volume']].copy()
            result['stock_id'] = stock_id
            result['market'] = 'TWSE'
            result['source'] = 'TWSE_STOCK_DAY'
            
            return result
            
        except Exception as e:
            logger.error(f"ç²å–TWSEæ•¸æ“šå¤±æ•— {stock_id} {year}/{month}: {e}")
            return None
    
    def fetch_tpex_stock_data(self, stock_id: str, year: int, month: int) -> Optional[pd.DataFrame]:
        """
        å¾TPExç²å–å–®ä¸€è‚¡ç¥¨çš„æœˆåº¦æ­·å²è³‡æ–™
        
        Args:
            stock_id: è‚¡ç¥¨ä»£ç¢¼
            year: å¹´ä»½ 
            month: æœˆä»½
            
        Returns:
            DataFrame æˆ– None
        """
        logger.info(f"ğŸš€ TPEx ä½¿ç”¨ FinMind ç©©å®šæ–¹æ¡ˆ: {stock_id} {year}/{month}")
        
        # ç›´æ¥ä½¿ç”¨ FinMind - TPEx å®˜æ–¹ API å·²æå£ï¼Œé€™æ˜¯æœ€ç©©å®šçš„æ–¹æ¡ˆ
        return self.fetch_tpex_finmind_backup(stock_id, year, month)

        # ä»¥ä¸‹æ˜¯åŸå§‹ç¨‹å¼ç¢¼ï¼ˆå·²åœç”¨ï¼‰
        try:
            # TPEx å€‹è‚¡æ—¥æˆäº¤è³‡è¨Šï¼ˆå·²çŸ¥æå£ï¼‰
            url = "https://www.tpex.org.tw/web/stock/aftertrading/daily_close_quotes/stk_quote_result.php"
            
            # è½‰æ›ç‚ºæ°‘åœ‹å¹´
            roc_year = year - 1911
            
            params = {
                'l': 'zh-tw',
                'o': 'json',
                'd': f"{roc_year}/{month:02d}",  # æ°‘åœ‹å¹´æ ¼å¼
                'stkno': stock_id
            }
            
            rate_limiter.acquire()  # å…¨åŸŸé€Ÿç‡é™åˆ¶
            response = self.session.get(url, params=params, timeout=15)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get('stat') != 'OK':
                logger.warning(f"TPEx JSON APIè¿”å›éOKç‹€æ…‹: {stock_id} {year}/{month}ï¼Œå˜—è©¦CSVå‚™æ´")
                # å˜—è©¦CSVå‚™æ´
                return self.fetch_tpex_stock_data_csv_fallback(stock_id, year, month)
                
            if not data.get('aaData'):
                logger.debug(f"ç„¡æ•¸æ“š: {stock_id} {year}/{month}")
                return None
            
            # è§£ææ•¸æ“š
            rows = []
            for row in data['aaData']:
                if len(row) >= 6:
                    # TPExæ ¼å¼: [æ—¥æœŸ, é–‹ç›¤, æœ€é«˜, æœ€ä½, æ”¶ç›¤, æˆäº¤é‡, ...]
                    date_str = f"{year}-{row[0].replace('/', '-')}"
                    rows.append({
                        'date': date_str,
                        'open': row[1],
                        'high': row[2], 
                        'low': row[3],
                        'close': row[4],
                        'volume': row[5] if len(row) > 5 else 0
                    })
            
            if not rows:
                return None
                
            df = pd.DataFrame(rows)
            
            # æ•¸æ“šæ¸…ç†
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
            
            # è½‰æ›æ•¸å­—æ¬„ä½ (æ›´å®‰å…¨çš„æ•¸å€¼æ¸…æ´—ï¼Œä¿ç•™åŸå§‹å€¼)
            numeric_cols = ['open', 'high', 'low', 'close', 'volume']
            for col in numeric_cols:
                # ä¿ç•™åŸå§‹å€¼ç”¨æ–¼è¿½æº¯
                df[f'{col}_raw'] = df[col].copy()
                
                # æ¸…æ´—ä¸¦è½‰æ›
                s = df[col].astype(str)
                s = s.str.replace(",", "", regex=False)
                s = s.replace({"--": None, "â€”": None, "": None, "nan": None})
                df[col] = pd.to_numeric(s, errors="coerce")
            
            # éæ¿¾ç„¡æ•ˆæ•¸æ“š
            df = df.dropna(subset=['date', 'open', 'high', 'low', 'close'])
            df = df[df['close'] > 0]
            
            if len(df) == 0:
                return None
                
            df['stock_id'] = stock_id
            df['market'] = 'TPEx'
            df['source'] = 'TPEX_JSON'
            
            return df[['date', 'open', 'high', 'low', 'close', 'volume', 'stock_id', 'market', 'source']]
            
        except Exception as e:
            logger.error(f"ç²å–TPExæ•¸æ“šå¤±æ•— {stock_id} {year}/{month}: {e}")
            # ç›´æ¥ä½¿ç”¨ FinMind å‚™æ´
            return self.fetch_tpex_finmind_backup(stock_id, year, month)

    def fetch_tpex_finmind_backup(self, stock_id: str, year: int, month: int) -> Optional[pd.DataFrame]:
        """TPEx FinMind å‚™æ´æ©Ÿåˆ¶ - ç©©å®šå¯é çš„æ­·å²æ•¸æ“š"""
        try:
            logger.info(f"ğŸš€ ä½¿ç”¨ FinMind å‚™æ´ç²å– {stock_id} {year}/{month}")
            
            # FinMind API
            url = "https://api.finmindtrade.com/api/v4/data"
            
            # è¨ˆç®—æ—¥æœŸç¯„åœ
            start_date = f"{year}-{month:02d}-01"
            
            # è¨ˆç®—æœˆæœ«
            if month == 12:
                next_year = year + 1
                next_month = 1
            else:
                next_year = year
                next_month = month + 1
            
            from datetime import datetime, timedelta
            last_day = datetime(next_year, next_month, 1) - timedelta(days=1)
            end_date = last_day.strftime('%Y-%m-%d')
            
            params = {
                'dataset': 'TaiwanStockPrice',
                'data_id': stock_id,
                'start_date': start_date,
                'end_date': end_date
            }
            
            logger.debug(f"FinMind åƒæ•¸: {params}")
            
            rate_limiter.acquire()  # å…¨åŸŸé€Ÿç‡é™åˆ¶
            response = self.session.get(url, params=params, timeout=15)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get('status') != 200:
                logger.warning(f"FinMind API éæˆåŠŸç‹€æ…‹: {data.get('status')} - {data.get('msg', '')}")
                return None
                
            if not data.get('data'):
                logger.warning(f"FinMind ç„¡æ•¸æ“š: {stock_id} {year}/{month}")
                return None
            
            finmind_data = data['data']
            logger.info(f"ğŸ“Š FinMind ç²å– {len(finmind_data)} ç­†æ•¸æ“š")
            
            # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
            rows = []
            for item in finmind_data:
                try:
                    rows.append({
                        'date': item['date'],
                        'open': float(item['open']),
                        'high': float(item['max']),  # FinMind ä½¿ç”¨ 'max'
                        'low': float(item['min']),   # FinMind ä½¿ç”¨ 'min'  
                        'close': float(item['close']),
                        'volume': int(item['Trading_Volume']) if item['Trading_Volume'] else 0
                    })
                except (KeyError, ValueError, TypeError) as e:
                    logger.debug(f"FinMind æ•¸æ“šè§£æè·³é: {item} - {e}")
                    continue
            
            if not rows:
                logger.warning(f"FinMind æ•¸æ“šè§£æå¾Œç‚ºç©º: {stock_id}")
                return None
                
            df = pd.DataFrame(rows)
            
            # æ•¸æ“šæ¸…ç†
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
            df = df.dropna(subset=['date', 'open', 'high', 'low', 'close'])
            df = df[df['close'] > 0]
            df = df.sort_values('date').reset_index(drop=True)
            
            if len(df) == 0:
                logger.warning(f"FinMind æ•¸æ“šæ¸…æ´—å¾Œç‚ºç©º: {stock_id}")
                return None
                
            # æ·»åŠ ä¾†æºæ¨™è¨˜
            result = df[['date', 'open', 'high', 'low', 'close', 'volume']].copy()
            result['stock_id'] = stock_id
            result['market'] = 'TPEx'
            result['source'] = 'FINMIND_BACKUP'
            
            logger.info(f"âœ… FinMind æˆåŠŸ: {stock_id} {year}/{month}, {len(result)} ç­†")
            return result
            
        except Exception as e:
            logger.error(f"FinMind å‚™æ´å¤±æ•— {stock_id} {year}/{month}: {e}")
            # æœ€å¾Œå˜—è©¦ CSV å‚™æ´
            return self.fetch_tpex_stock_data_csv_fallback(stock_id, year, month)

    def fetch_tpex_stock_data_csv_fallback(self, stock_id: str, year: int, month: int) -> Optional[pd.DataFrame]:
        """TPEx CSV å‚™æ´æ©Ÿåˆ¶ (æ°‘åœ‹å¹´æ ¼å¼)"""
        try:
            # è½‰æ›ç‚ºæ°‘åœ‹å¹´
            roc_year = year - 1911
            
            # TPEx CSV ä¸‹è¼‰ URL (æ°‘åœ‹å¹´æ ¼å¼)
            url = "https://www.tpex.org.tw/web/stock/historical_trading/stk_quote_download.php"
            params = {
                "l": "zh-tw",
                "d": f"{roc_year}/{month:02d}",
                "stkno": stock_id,
                "download": "csv"
            }
            
            logger.info(f"å˜—è©¦ TPEx CSV å‚™æ´: {stock_id} {roc_year}/{month:02d}")
            
            rate_limiter.acquire()  # å…¨åŸŸé€Ÿç‡é™åˆ¶
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºCSVå…§å®¹
            content_type = response.headers.get('content-type', '')
            if 'text/csv' not in content_type and 'application/csv' not in content_type:
                logger.warning(f"TPEx CSV å›æ‡‰é¡å‹ç•°å¸¸: {content_type}")
            
            # è§£æ CSV å…§å®¹
            lines = response.text.strip().split('\n')
            if len(lines) < 2:
                logger.warning(f"TPEx CSV ç„¡è³‡æ–™: {stock_id} {year}-{month:02d}")
                return None
            
            rows = []
            header = lines[0].split(',')
            
            # æ›´éŸŒæ€§çš„ TPEx CSV æ¬„ä½åæŠ“å–
            def _find_col(cols, keys):
                """ä»¥åŒ…å«å¼æ¯”å°æ‰¾æ¬„ä½åï¼Œæ›´éŸŒæ€§è™•ç†å¾®èª¿"""
                for k in keys:
                    for c in cols:
                        if k in c:
                            return c
                return None
            
            try:
                # ä½¿ç”¨åŒ…å«å¼åŒ¹é…æ‰¾æ¬„ä½å (è¿”å›æ¬„ä½åè€Œéç´¢å¼•)
                date_col = _find_col(header, ['æ—¥æœŸ', 'æˆäº¤æ—¥æœŸ'])
                open_col = _find_col(header, ['é–‹ç›¤'])
                high_col = _find_col(header, ['æœ€é«˜'])
                low_col = _find_col(header, ['æœ€ä½'])
                close_col = _find_col(header, ['æ”¶ç›¤'])
                vol_col = _find_col(header, ['æˆäº¤è‚¡æ•¸', 'æˆäº¤è‚¡æ•¸(åƒè‚¡)'])
                
                # è½‰æ›ç‚ºç´¢å¼•
                date_idx = header.index(date_col) if date_col else None
                open_idx = header.index(open_col) if open_col else None
                high_idx = header.index(high_col) if high_col else None
                low_idx = header.index(low_col) if low_col else None
                close_idx = header.index(close_col) if close_col else None
                volume_idx = header.index(vol_col) if vol_col else None
                        
                if None in [date_idx, open_idx, high_idx, low_idx, close_idx, volume_idx]:
                    logger.error(f"TPEx CSV æ‰¾ä¸åˆ°å¿…è¦æ¬„ä½: {header}")
                    logger.error(f"åŒ¹é…çµæœ: date={date_idx}, open={open_idx}, high={high_idx}, low={low_idx}, close={close_idx}, volume={volume_idx}")
                    return None
                    
            except Exception as e:
                logger.error(f"TPEx CSV æ¬„ä½è§£æç•°å¸¸: {e}")
                return None
            
            for line in lines[1:]:
                if not line.strip():
                    continue
                    
                fields = line.split(',')
                if len(fields) != len(header):
                    continue
                
                try:
                    # è§£ææ—¥æœŸ (æ°‘åœ‹å¹´ xxx/mm/dd æ ¼å¼)
                    date_str = fields[date_idx].strip()
                    if '/' not in date_str:
                        continue
                    
                    roc_parts = date_str.split('/')
                    if len(roc_parts) != 3:
                        continue
                    
                    roc_year_str, month_str, day_str = roc_parts
                    ad_year = int(roc_year_str) + 1911
                    date_obj = datetime(ad_year, int(month_str), int(day_str))
                    
                    # è§£æåƒ¹æ ¼ (å»é™¤åƒåˆ†ä½é€—è™Ÿ)
                    def parse_price(value_str):
                        cleaned = value_str.replace(',', '').replace('--', '').replace('â€”', '').strip()
                        return float(cleaned) if cleaned and cleaned != '0' else None
                    
                    open_price = parse_price(fields[open_idx])
                    high_price = parse_price(fields[high_idx])
                    low_price = parse_price(fields[low_idx])
                    close_price = parse_price(fields[close_idx])
                    
                    # æˆäº¤é‡ (åƒè‚¡è½‰ç‚ºè‚¡)
                    volume_str = fields[volume_idx].replace(',', '').strip()
                    volume = int(float(volume_str) * 1000) if volume_str and volume_str not in ['--', 'â€”'] else 0
                    
                    # åŸºæœ¬è³‡æ–™é©—è­‰
                    if not all([open_price, high_price, low_price, close_price]):
                        continue
                    if high_price < max(open_price, close_price):
                        continue
                    if low_price > min(open_price, close_price):
                        continue
                    
                    rows.append({
                        'date': date_obj,
                        'open': open_price,
                        'high': high_price,
                        'low': low_price,
                        'close': close_price,
                        'volume': volume
                    })
                    
                except (ValueError, IndexError) as e:
                    logger.debug(f"TPEx CSV è§£æå¤±æ•—è¡Œ: {line[:50]}, error: {e}")
                    continue
            
            if not rows:
                logger.warning(f"TPEx CSV å‚™æ´ç„¡æœ‰æ•ˆè³‡æ–™: {stock_id} {year}-{month:02d}")
                return None
                
            df = pd.DataFrame(rows)
            df['stock_id'] = stock_id
            df['market'] = 'TPEx'
            df['source'] = 'TPEX_CSV_BACKUP'
            
            logger.info(f"TPEx CSV å‚™æ´æˆåŠŸ: {stock_id} {year}-{month:02d}, å–å¾— {len(rows)} ç­†")
            return df[['date', 'open', 'high', 'low', 'close', 'volume', 'stock_id', 'market', 'source']]
            
        except Exception as e:
            logger.error(f"TPEx CSV å‚™æ´å¤±æ•— {stock_id} {year}-{month:02d}: {e}")
            return None
    
    def create_price_tables(self):
        """å»ºç«‹åƒ¹æ ¼æ•¸æ“šè¡¨çµæ§‹å’Œç´¢å¼•å„ªåŒ–"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ç¢ºä¿ daily_prices è¡¨å­˜åœ¨
            ensure_daily_prices_table(conn)
            
            # Cç´šå„ªåŒ–: å»ºç«‹ç´¢å¼•
            ensure_optimized_indexes(conn)
            
            # æª¢æŸ¥å·²å­˜åœ¨çš„åƒ¹æ ¼è¡¨
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name LIKE 'stock_%'")
            existing_tables = [row[0] for row in cursor.fetchall()]
            
            logger.info(f"ç¾æœ‰åƒ¹æ ¼è¡¨æ•¸é‡: {len(existing_tables)}")
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"æª¢æŸ¥åƒ¹æ ¼è¡¨å¤±æ•—: {e}")
    
    def save_stock_price_data(self, stock_id: str, market: str, source: str, df: pd.DataFrame):
        """ä¿å­˜è‚¡ç¥¨åƒ¹æ ¼è³‡æ–™åˆ°å–®ä¸€äº‹å¯¦è¡¨ (æœå‹™ç´šæ¶æ§‹)"""
        conn = None
        try:
            # æ ¡é©—è‚¡ç¥¨ä»£ç¢¼
            stock_id = sanitize_stock_id(stock_id)
            
            conn = sqlite3.connect(self.db_path)
            # å„ªåŒ–SQLiteæ•ˆèƒ½
            conn.execute("PRAGMA journal_mode=WAL;")
            conn.execute("PRAGMA synchronous=NORMAL;")
            
            # ç¢ºä¿å–®è¡¨æ¶æ§‹å­˜åœ¨
            ensure_daily_prices_table(conn)
            
            # æ‰¹æ¬¡æ’å…¥æ•¸æ“š - ä½¿ç”¨ per-row ç²¾æº–ä¾†æºè¿½æº¯ (å¿…è£œé …ç›®2)
            rows = []
            for r in df.itertuples(index=False):
                try:
                    # ç¢ºä¿æ‰€æœ‰æ•¸å€¼éƒ½æ˜¯æœ‰æ•ˆçš„
                    if pd.notna(r.open) and pd.notna(r.high) and pd.notna(r.low) and pd.notna(r.close) and pd.notna(r.volume):
                        # å„ªå…ˆä½¿ç”¨ df å…§çš„ market/sourceï¼Œå›é€€åˆ°åƒæ•¸å‚³å…¥å€¼
                        row_market = getattr(r, 'market', market)
                        row_source = getattr(r, 'source', source)
                        
                        # è™•ç†æ—¥æœŸæ ¼å¼ï¼šæ”¯æ´å­—ç¬¦ä¸²å’Œdatetimeå…©ç¨®
                        date_str = r.date if isinstance(r.date, str) else r.date.strftime("%Y-%m-%d")
                        
                        rows.append((
                            stock_id,
                            date_str, 
                            float(r.open), 
                            float(r.high),
                            float(r.low), 
                            float(r.close), 
                            int(r.volume),
                            row_market,
                            row_source
                        ))
                except (ValueError, AttributeError, OverflowError) as e:
                    logger.warning(f"è·³éç„¡æ•ˆè¨˜éŒ„ {stock_id} {r.date}: {e}")
                    continue
            
            if rows:
                # å¯«å…¥æ•ˆèƒ½å¾®èª¿ (æ‡‰åŠ é …ç›®7)
                # æ‰¹æ¬¡å¯«å…¥ä»¥é™ä½ I/O é »ç‡
                batch_size = 1000
                for i in range(0, len(rows), batch_size):
                    batch = rows[i:i + batch_size]
                    conn.executemany("""
                        INSERT OR REPLACE INTO daily_prices
                        (stock_id, date, open, high, low, close, volume, market, source)
                        VALUES (?,?,?,?,?,?,?,?,?)
                    """, batch)
                    
                    # æ¯æ‰¹æ¬¡å¾ŒçŸ­æš«ä¼‘æ¯ï¼Œé¿å… I/O å³°å€¼
                    if i + batch_size < len(rows):
                        time.sleep(0.01)
                
                conn.commit()
                logger.debug(f"ä¿å­˜ {stock_id}: {len(rows)} ç­†è¨˜éŒ„åˆ°å–®è¡¨")
            else:
                logger.warning(f"è‚¡ç¥¨ {stock_id} ç„¡æœ‰æ•ˆè¨˜éŒ„å¯ä¿å­˜")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ {stock_id} åƒ¹æ ¼æ•¸æ“šå¤±æ•—: {e}")
        finally:
            if conn:
                conn.close()
    
    def fetch_market_daily_data(self, date: str) -> Optional[pd.DataFrame]:
        """
        å–å¾—TWSEå…¨å¸‚å ´ç‰¹å®šæ—¥æœŸçš„è‚¡åƒ¹è³‡æ–™ (Bç´šå„ªåŒ–)
        
        Args:
            date: æ—¥æœŸæ ¼å¼ YYYYMMDD (e.g., '20250909')
            
        Returns:
            DataFrame: åŒ…å«å…¨å¸‚å ´è‚¡ç¥¨çš„OHLCVè³‡æ–™
        """
        url = "https://www.twse.com.tw/exchangeReport/STOCK_DAY_ALL"
        params = {
            'response': 'json',
            'date': date
        }
        
        try:
            rate_limiter.acquire()  # å…¨åŸŸé€Ÿç‡é™åˆ¶
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            # æª¢æŸ¥APIç‹€æ…‹å’Œè³‡æ–™
            if data.get('stat') != 'OK':
                logger.warning(f"TWSEå…¨å¸‚å ´æ—¥å½™ç¸½APIç‹€æ…‹ç•°å¸¸: {data.get('stat')} - {date}")
                return None
                
            if 'data' not in data or not data['data']:
                logger.warning(f"TWSEå…¨å¸‚å ´æ—¥å½™ç¸½ç„¡è³‡æ–™: {date}")
                return None
            
            rows = []
            for row in data['data']:
                try:
                    # STOCK_DAY_ALL APIçš„æ¬„ä½æ ¼å¼ (æ ¹æ“šfieldsé †åº)ï¼š
                    # [0]è­‰åˆ¸ä»£è™Ÿ, [1]è­‰åˆ¸åç¨±, [2]æˆäº¤è‚¡æ•¸, [3]æˆäº¤é‡‘é¡
                    # [4]é–‹ç›¤åƒ¹, [5]æœ€é«˜åƒ¹, [6]æœ€ä½åƒ¹, [7]æ”¶ç›¤åƒ¹
                    # [8]æ¼²è·Œåƒ¹å·®, [9]æˆäº¤ç­†æ•¸
                    
                    if len(row) < 10:
                        continue
                        
                    stock_code = str(row[0]).strip()
                    
                    # éæ¿¾é4ä½æ•¸è‚¡ç¥¨ä»£è™Ÿ
                    if not re.match(r'^\d{4}$', stock_code):
                        continue
                    
                    # è§£æåƒ¹æ ¼æ¬„ä½
                    def parse_price_safe(price_str):
                        if not price_str or price_str in ['--', 'X', '', 'é™¤æ¬Š']:
                            return None
                        try:
                            # ç§»é™¤é€—è™Ÿå’Œç‰¹æ®Šå­—å…ƒ
                            clean_str = str(price_str).replace(',', '').replace('X', '').replace('+', '').replace('-', '').strip()
                            return float(clean_str) if clean_str else None
                        except (ValueError, AttributeError):
                            return None
                    
                    open_price = parse_price_safe(row[4])   # é–‹ç›¤åƒ¹
                    high_price = parse_price_safe(row[5])   # æœ€é«˜åƒ¹
                    low_price = parse_price_safe(row[6])    # æœ€ä½åƒ¹
                    close_price = parse_price_safe(row[7])  # æ”¶ç›¤åƒ¹
                    
                    # è§£ææˆäº¤é‡ (å·²ç¶“æ˜¯è‚¡æ•¸ï¼Œä¸éœ€è¦Ã—1000)
                    volume_str = str(row[2]).replace(',', '').strip()
                    try:
                        volume = int(float(volume_str)) if volume_str and volume_str not in ['--', ''] else 0
                    except (ValueError, AttributeError):
                        volume = 0
                    
                    # è³‡æ–™é©—è­‰
                    if not all([open_price, high_price, low_price, close_price]):
                        continue
                    if high_price < max(open_price, close_price, low_price):
                        continue
                    if low_price > min(open_price, close_price, high_price):
                        continue
                    
                    # è½‰æ›æ—¥æœŸæ ¼å¼ YYYYMMDD -> YYYY-MM-DD
                    date_formatted = f"{date[:4]}-{date[4:6]}-{date[6:8]}"
                    
                    rows.append({
                        'stock_id': stock_code,
                        'date': date_formatted,
                        'open': open_price,
                        'high': high_price,
                        'low': low_price,
                        'close': close_price,
                        'volume': volume,
                        'market': 'TWSE',
                        'source': 'TWSE_DAILY_ALL'
                    })
                    
                except (ValueError, IndexError, TypeError) as e:
                    logger.debug(f"TWSEå…¨å¸‚å ´æ—¥å½™ç¸½è§£æå¤±æ•—è¡Œ: {row[:3] if len(row) > 2 else row}, error: {e}")
                    continue
            
            if not rows:
                logger.warning(f"TWSEå…¨å¸‚å ´æ—¥å½™ç¸½ç„¡æœ‰æ•ˆè³‡æ–™: {date}")
                return None
            
            df = pd.DataFrame(rows)
            logger.info(f"âœ… TWSEå…¨å¸‚å ´æ—¥å½™ç¸½ {date}: {len(df)} æª”è‚¡ç¥¨")
            return df
            
        except requests.exceptions.RequestException as e:
            logger.error(f"TWSEå…¨å¸‚å ´æ—¥å½™ç¸½ç¶²è·¯éŒ¯èª¤ {date}: {e}")
            return None
        except (KeyError, ValueError, json.JSONDecodeError) as e:
            logger.error(f"TWSEå…¨å¸‚å ´æ—¥å½™ç¸½è§£æéŒ¯èª¤ {date}: {e}")
            return None
    
    def get_recent_trading_dates(self, days: int = 60) -> List[str]:
        """
        ç²å–æœ€è¿‘Nå€‹äº¤æ˜“æ—¥(æ’é™¤å‡æ—¥) - Bç´šå„ªåŒ–è¼”åŠ©å‡½æ•¸
        
        Args:
            days: éœ€è¦çš„äº¤æ˜“æ—¥æ•¸é‡
            
        Returns:
            List[str]: äº¤æ˜“æ—¥æ¸…å–®ï¼Œæ ¼å¼YYYYMMDD
        """
        from datetime import datetime, timedelta
        
        trading_dates = []
        current_date = datetime.now()
        days_checked = 0
        max_checks = days * 2  # é˜²æ­¢ç„¡é™è¿´åœˆ
        
        while len(trading_dates) < days and days_checked < max_checks:
            # è·³éé€±æœ«
            if current_date.weekday() < 5:  # 0-6, é€±ä¸€åˆ°é€±äº”
                date_str = current_date.strftime('%Y%m%d')
                trading_dates.append(date_str)
            
            current_date -= timedelta(days=1)
            days_checked += 1
        
        return trading_dates[:days]  # æœ€æ–°çš„åœ¨å‰
    
    def fetch_market_recent_data_batch(self, target_bars: int = 60) -> Dict[str, pd.DataFrame]:
        """
        Bç´šå„ªåŒ–ï¼šæ‰¹æ¬¡æŠ½å–è¿‘æœŸå…¨å¸‚å ´è³‡æ–™
        å–ä»£é€æª”é€æœˆçš„å‚³çµ±æ–¹å¼ï¼Œç”¨æ—¥å½™ç¸½APIä¸€æ¬¡å–å¾—å¤šæª”è‚¡ç¥¨
        
        Args:
            target_bars: ç›®æ¨™Kç·šæ ¹æ•¸
            
        Returns:
            Dict[str, pd.DataFrame]: {stock_id: DataFrame}
        """
        logger.info(f"ğŸš€ Bç´šå„ªåŒ–ï¼šé–‹å§‹å…¨å¸‚å ´æ‰¹æ¬¡æŠ½å– {target_bars} å€‹äº¤æ˜“æ—¥è³‡æ–™")
        
        # 1. ç²å–äº¤æ˜“æ—¥æ¸…å–®
        trading_dates = self.get_recent_trading_dates(target_bars + 10)  # å¤šå–10å¤©ä¿é™©
        logger.info(f"ğŸ“… å°‡æŠ½å– {len(trading_dates)} å€‹äº¤æ˜“æ—¥: {trading_dates[:5]}...{trading_dates[-3:]}")
        
        # 2. é€æ—¥æŠ½å–å…¨å¸‚å ´è³‡æ–™
        all_stock_data = {}
        successful_dates = 0
        
        for i, date in enumerate(trading_dates):
            logger.info(f"ğŸ“ æŠ½å– {i+1}/{len(trading_dates)}: {date}")
            
            daily_data = self.fetch_market_daily_data(date)
            if daily_data is not None and len(daily_data) > 0:
                successful_dates += 1
                
                # å°‡è³‡æ–™æŒ‰stock_idåˆ†çµ„
                for _, row in daily_data.iterrows():
                    stock_id = row['stock_id']
                    if stock_id not in all_stock_data:
                        all_stock_data[stock_id] = []
                    all_stock_data[stock_id].append(row.to_dict())
            else:
                logger.warning(f"âš ï¸ {date} ç„¡è³‡æ–™æˆ–æŠ½å–å¤±æ•—")
        
        # 3. è½‰æ›DataFrameä¸¦ç¯©é¸
        final_stock_data = {}
        for stock_id, records in all_stock_data.items():
            if len(records) >= target_bars:  # ç¢ºä¿æœ‰è¶³å¤ çš„Kç·š
                df = pd.DataFrame(records)
                df = df.sort_values('date').tail(target_bars)  # å–æœ€è¿‘target_barsç­†
                final_stock_data[stock_id] = df
        
        logger.info(f"âœ… Bç´šå„ªåŒ–å®Œæˆï¼šæˆåŠŸæŠ½å– {successful_dates}/{len(trading_dates)} å€‹äº¤æ˜“æ—¥ï¼Œå¾—åˆ° {len(final_stock_data)} æª”è‚¡ç¥¨")
        return final_stock_data
    
    def fetch_stock_historical_data(self, stock_id: str, market: str, target_bars: int = 60) -> bool:
        """
        ç²å–è‚¡ç¥¨æ­·å²è³‡æ–™ (ç²¾ç¢ºæ§åˆ¶Kç·šæ ¹æ•¸) - ä¿®æ­£ç‰ˆ
        
        Args:
            stock_id: è‚¡ç¥¨ä»£ç¢¼
            market: å¸‚å ´ (TWSE/TPEx)
            target_bars: ç›®æ¨™Kç·šæ ¹æ•¸ (é è¨­60æ ¹)
            
        Returns:
            æˆåŠŸèˆ‡å¦
        """
        all_data = []
        
        try:
            current_date = datetime.now()
            # å‹•æ…‹è¨ˆç®—éœ€è¦æœˆæ•¸ï¼šä¸€å€‹æœˆç´„18äº¤æ˜“æ—¥
            need_months = max(3, math.ceil(target_bars / 18) + 1)
            months_tried = 0
            total_records = 0
            
            # ç¬¬ä¸€è¼ªï¼šæŒ‰é ä¼°æœˆæ•¸æŠ“å–
            while total_records < target_bars and months_tried < need_months:
                year = current_date.year
                month = current_date.month
                months_tried += 1
                
                logger.debug(f"ç²å– {stock_id} {year}/{month} (ç›®æ¨™: {target_bars}æ ¹, å·²ç²å–: {len(all_data)}æ ¹)")
                
                # æ ¹æ“šå¸‚å ´é¸æ“‡API
                if market == 'TWSE':
                    df = self.fetch_twse_stock_data(stock_id, year, month)
                else:  # TPEx
                    df = self.fetch_tpex_stock_data(stock_id, year, month)
                
                if df is not None and len(df) > 0:
                    all_data.append(df)
                    total_records += len(df)
                    logger.info(f"âœ… {stock_id} {year}/{month}: {len(df)} ç­† (ç´¯è¨ˆ: {total_records})")
                else:
                    logger.warning(f"âŒ {stock_id} {year}/{month}: ç„¡æ•¸æ“š")
                
                # é—œéµä¿®æ­£ï¼šå¾€å‰ç§»å‹•ä¸€å€‹æœˆ
                current_date = current_date - relativedelta(months=1)
                
                # å·²æœ‰å…¨åŸŸé€Ÿç‡é™åˆ¶ï¼Œç§»é™¤å›ºå®šå»¶é²
            
            # ç¬¬äºŒè¼ªï¼šå¦‚æœé‚„ä¸å¤ ï¼Œå†æœ€å¤šæŠ“6å€‹æœˆä½œç‚ºä¿éšª
            max_extra_months = 6
            while total_records < target_bars and months_tried < need_months + max_extra_months:
                year = current_date.year
                month = current_date.month
                months_tried += 1
                
                logger.debug(f"ä¿éšªè¼ª: ç²å– {stock_id} {year}/{month}")
                
                if market == 'TWSE':
                    df = self.fetch_twse_stock_data(stock_id, year, month)
                else:
                    df = self.fetch_tpex_stock_data(stock_id, year, month)
                
                if df is not None and len(df) > 0:
                    all_data.append(df)
                    total_records += len(df)
                    logger.info(f"âœ… {stock_id} {year}/{month}: {len(df)} ç­† (ä¿éšªè¼ªç´¯è¨ˆ: {total_records})")
                
                current_date = current_date - relativedelta(months=1)
                time.sleep(0.5)
            
            if not all_data:
                logger.warning(f"âŒ è‚¡ç¥¨ {stock_id} ç„¡æ³•ç²å–ä»»ä½•æ•¸æ“š")
                return False
            
            # åˆä½µæ‰€æœ‰DataFrame
            df_all = pd.concat(all_data, ignore_index=True)
            df_all['date'] = pd.to_datetime(df_all['date'])
            
            # å»é‡èˆ‡æ’åº
            df_all = df_all.dropna(subset=['date', 'open', 'high', 'low', 'close'])
            
            # é¿å…æœªä¾†æ—¥èª¤å…¥åº« - å°åŒ—æ™‚å€ä¸€è‡´åŒ– (å¿…è£œé …ç›®1)
            today = pd.Timestamp.now(tz=TAIPEI).normalize().tz_localize(None)
            future_dates = df_all[df_all['date'] > today]
            if len(future_dates) > 0:
                logger.warning(f"âš ï¸ {stock_id} ç™¼ç¾ {len(future_dates)} ç­†æœªä¾†æ—¥æœŸï¼Œå·²éæ¿¾: {future_dates['date'].tolist()}")
                df_all = df_all[df_all['date'] <= today]
            
            df_all = df_all.drop_duplicates(subset=['date']).sort_values('date').reset_index(drop=True)
            
            # ç•°å¸¸é‡åƒ¹ä¿å®ˆè™•ç† (æ‡‰åŠ é …ç›®4)
            df_all = df_all.dropna(subset=['open', 'high', 'low', 'close'])
            df_all = df_all[df_all['close'] > 0]
            # è‹¥ä¸æƒ³æŠŠåœç‰Œæ—¥å¯«å…¥ï¼Œå–æ¶ˆä¸‹è¡Œæ³¨é‡‹ï¼š
            # df_all = df_all[df_all['volume'].fillna(0) > 0]
            
            # æŠ“ä¸æ»¿çš„æ˜ç¢ºå‘Šè­¦ (æ‡‰åŠ é …ç›®5)
            if len(df_all) < target_bars:
                logger.warning(f"âš ï¸ {stock_id} åƒ… {len(df_all)}/{target_bars} æ ¹ï¼ˆmonths_tried={months_tried}ï¼‰")
            
            if len(df_all) > target_bars:
                df_all = df_all.tail(target_bars).reset_index(drop=True)
            
            # ä¿å­˜åˆ°è³‡æ–™åº« (å–®è¡¨æ¶æ§‹)
            source = f"{market}_API"
            self.save_stock_price_data(stock_id, market, source, df_all)
            
            # ä¿®æ­£æ—¥å¿—æ ¼å¼
            start = df_all.iloc[0]['date'].strftime('%Y-%m-%d')
            end = df_all.iloc[-1]['date'].strftime('%Y-%m-%d')
            logger.info(f"âœ… è‚¡ç¥¨ {stock_id} å®Œæˆ: å…± {len(df_all)} ç­† ({start} ~ {end})")
            return True
        
        except Exception as e:
            logger.error(f"ç²å– {stock_id} æ­·å²è³‡æ–™å¤±æ•—: {e}")
            return False
    
    def optimize_db_for_bulk_insert(self, conn):
        """
        Cç´šå„ªåŒ–ï¼šè¨­ç½®SQLçš„PRAGMAé€²è¡Œå¤§é‡åŒ¯å…¥å„ªåŒ–
        """
        logger.info("ğŸ› ï¸ Cç´šå„ªåŒ–: è¨­å®šé«˜æ€§èƒ½PRAGMA")
        
        # é«˜æ€§èƒ½å¯«å…¥æ¨¡å¼
        conn.execute("PRAGMA journal_mode=WAL")
        conn.execute("PRAGMA synchronous=NORMAL")  # å¾ FULL é™è‡³ NORMAL
        conn.execute("PRAGMA temp_store=MEMORY")    # æš«å­˜è¨˜æ†¶é«”
        conn.execute("PRAGMA cache_size=-200000")   # 200MB cache
        conn.execute("PRAGMA mmap_size=268435456")  # 256MB memory mapping
        
        logger.info("âœ… PRAGMAè¨­å®šå®Œæˆï¼šå·²å„ªåŒ–è³‡æ–™åº«å¯«å…¥æ€§èƒ½")
    
    def restore_db_settings(self, conn):
        """
        Cç´šå„ªåŒ–ï¼šé‚„åŸæ­£å¸¸çš„PRAGMAè¨­å®š
        """
        logger.info("ğŸ”„ é‚„åŸæ­£å¸¸è³‡æ–™åº«è¨­å®š...")
        
        conn.execute("PRAGMA synchronous=FULL")   # é‚„åŸç‚ºæœ€å®‰å…¨æ¨¡å¼
        conn.execute("PRAGMA cache_size=2000")     # é‚„åŸé è¨­å€¼
        conn.execute("PRAGMA mmap_size=0")         # é—œé–‰memory mapping
        
        logger.info("âœ… è³‡æ–™åº«è¨­å®šå·²é‚„åŸ")
    
    def batch_insert_stock_data(self, all_stock_data: Dict[str, pd.DataFrame]):
        """
        Cç´šå„ªåŒ–ï¼šæ‰¹æ¬¡å¯«å…¥è‚¡ç¥¨è³‡æ–™ï¼Œä½¿ç”¨å¤§transaction
        
        Args:
            all_stock_data: {stock_id: DataFrame} çš„è³‡æ–™
        
        Returns:
            Tuple[int, int]: (success_count, failed_count)
        """
        logger.info(f"ğŸ“¦ Cç´šå„ªåŒ–: æ‰¹æ¬¡å¯«å…¥ {len(all_stock_data)} æª”è‚¡ç¥¨è³‡æ–™")
        
        conn = None
        success = 0
        failed = 0
        
        try:
            # é€£ç·šè³‡æ–™åº«ä¸¦å„ªåŒ–
            conn = sqlite3.connect(self.db_path)
            self.optimize_db_for_bulk_insert(conn)
            
            # é–‹å§‹å¤§transaction
            conn.execute("BEGIN TRANSACTION")
            logger.info("ğŸš€ é–‹å§‹å¤§transactionæ‰¹æ¬¡å¯«å…¥...")
            
            # æº–å‚™æ‰¹æ¬¡æ’å…¥èªå¥
            insert_sql = """
                INSERT OR REPLACE INTO daily_prices 
                (stock_id, date, open, high, low, close, volume, market, source, ingested_at) 
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))
            """
            
            # æ”¶é›†æ‰€æœ‰è³‡æ–™ç‚ºä¸€å€‹å¤§æ‰¹æ¬¡
            all_rows = []
            
            for stock_id, df in all_stock_data.items():
                try:
                    # æª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–° (Dç´šå„ªåŒ–çš„ç°¡åŒ–ç‰ˆ)
                    if self.is_fresh_enough(stock_id, len(df)):
                        logger.debug(f"â†ªï¸ è·³é {stock_id}ï¼ˆè³‡æ–™å¤ æ–°ï¼‰")
                        success += 1
                        continue
                    
                    # æº–å‚™è©²è‚¡ç¥¨çš„æ‰€æœ‰è³‡æ–™
                    for _, row in df.iterrows():
                        # è™•ç†æ—¥æœŸæ ¼å¼
                        date_str = row['date'] if isinstance(row['date'], str) else row['date'].strftime('%Y-%m-%d')
                        
                        all_rows.append((
                            stock_id,
                            date_str,
                            float(row['open']),
                            float(row['high']),
                            float(row['low']),
                            float(row['close']),
                            int(row['volume']),
                            row['market'],
                            row['source']
                        ))
                    
                    success += 1
                    
                except Exception as e:
                    logger.error(f"âŒ æº–å‚™ {stock_id} è³‡æ–™å¤±æ•—: {e}")
                    failed += 1
                    continue
            
            # ä¸€æ¬¡æ€§æ‰¹æ¬¡æ’å…¥æ‰€æœ‰è³‡æ–™
            if all_rows:
                logger.info(f"ğŸ“¥ åŸ·è¡Œæ‰¹æ¬¡æ’å…¥: {len(all_rows)} ç­†è¨˜éŒ„")
                conn.executemany(insert_sql, all_rows)
                logger.info(f"âœ… æ‰¹æ¬¡æ’å…¥å®Œæˆ")
            
            # æäº¤å¤§transaction
            conn.commit()
            logger.info(f"ğŸ‰ å¤§transactionæäº¤æˆåŠŸ: {len(all_rows)} ç­†è¨˜éŒ„")
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡å¯«å…¥å¤±æ•—: {e}")
            if conn:
                conn.rollback()
                logger.info("ğŸ”„ å·²å›æ»¾äº¤æ˜“")
            # å…¨éƒ¨è¨˜ç‚ºå¤±æ•—
            failed = len(all_stock_data)
            success = 0
        
        finally:
            if conn:
                # é‚„åŸè³‡æ–™åº«è¨­å®š
                self.restore_db_settings(conn)
                conn.close()
        
        return success, failed
    
    def run_price_data_pipeline_optimized(self, max_stocks: Optional[int] = None, target_bars: int = 60, specific_stocks: Optional[List[str]] = None):
        """
        åŸ·è¡Œåƒ¹æ ¼æ•¸æ“šç®¡é“ - Bç´šå„ªåŒ–ç‰ˆ (æ—¥å½™ç¸½æ‰¹æ¬¡æŠ½å–)
        å–ä»£é€æª”é€æœˆçš„å‚³çµ±æ–¹å¼ï¼Œé€Ÿåº¦æå‡ 5-10 å€
        
        Args:
            max_stocks: æœ€å¤§è™•ç†è‚¡ç¥¨æ•¸ (None = å…¨éƒ¨)
            target_bars: ç›®æ¨™Kç·šæ ¹æ•¸ (é è¨­60æ ¹)
            specific_stocks: æŒ‡å®šè‚¡ç¥¨æ¸…å–® (None = ä½¿ç”¨é è¨­æ¸…å–®)
        """
        logger.info("ğŸš€ åŸ·è¡Œ Bç´šå„ªåŒ–ç‰ˆåƒ¹æ ¼æ•¸æ“šç®¡é“...")
        start_time = datetime.now()
        
        # å»ºç«‹è¡¨çµæ§‹
        self.create_price_tables()
        
        # 1. ä½¿ç”¨å…¨å¸‚å ´æ‰¹æ¬¡æŠ½å– (Bç´šå„ªåŒ–æ ¸å¿ƒ)
        logger.info("ğŸ† Bç´šå„ªåŒ–: ä½¿ç”¨å…¨å¸‚å ´æ—¥å½™ç¸½APIæ‰¹æ¬¡æ‹½å–")
        market_data = self.fetch_market_recent_data_batch(target_bars)
        
        if not market_data:
            logger.error("âŒ Bç´šå„ªåŒ–å¤±æ•—: ç„¡æ³•ç²å–å¸‚å ´è³‡æ–™")
            return 0, 0
        
        # 2. éæ¿¾è‚¡ç¥¨ (å¦‚æœæŒ‡å®š)
        if specific_stocks:
            filtered_data = {k: v for k, v in market_data.items() if k in specific_stocks}
            logger.info(f"ğŸ¯ æŒ‡å®šè‚¡ç¥¨éæ¿¾: {len(filtered_data)}/{len(market_data)} æª”")
            market_data = filtered_data
        
        if max_stocks:
            # å–å‰ max_stocks æª”
            market_data = dict(list(market_data.items())[:max_stocks])
            logger.info(f"ğŸ¯ æ•¸é‡é™åˆ¶: å‰ {max_stocks} æª”")
        
        # 3. ä½¿ç”¨Cç´šå„ªåŒ–æ‰¹æ¬¡å¯«å…¥è³‡æ–™åº«
        logger.info("ğŸ† Cç´šå„ªåŒ–: ä½¿ç”¨å¤§transactionæ‰¹æ¬¡å¯«å…¥")
        success, failed = self.batch_insert_stock_data(market_data)
        
        # çµæœçµ±è¨ˆ
        total_time = (datetime.now() - start_time).seconds
        logger.info(f"ğŸ‰ Bç´šå„ªåŒ–ç‰ˆå®Œæˆ!")
        logger.info(f"ğŸ“ è™•ç†çµæœ: ç¸½è¨ˆ{len(market_data)}æª”, æˆåŠŸ{success}æª”, å¤±æ•—{failed}æª”")
        logger.info(f"â±ï¸  ç¸½è€—æ™‚: {total_time/60:.1f}åˆ†é˜ (å¹³å‡ {total_time/len(market_data):.1f}ç§’/æª”)")
        
        # é¡¯ç¤ºè³‡æ–™åº«çµ±è¨ˆ
        self.validate_price_data()
        
        return success, failed
    
    def run_price_data_pipeline(self, max_stocks: Optional[int] = None, target_bars: int = 60, specific_stocks: Optional[List[str]] = None):
        """
        åŸ·è¡Œåƒ¹æ ¼æ•¸æ“šç®¡é“
        
        Args:
            max_stocks: æœ€å¤§è™•ç†è‚¡ç¥¨æ•¸ (None = å…¨éƒ¨)
            target_bars: ç›®æ¨™Kç·šæ ¹æ•¸ (é è¨­60æ ¹)
            specific_stocks: æŒ‡å®šè‚¡ç¥¨æ¸…å–® (None = ä½¿ç”¨é è¨­æ¸…å–®)
        """
        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œåƒ¹æ ¼æ•¸æ“šç®¡é“...")
        
        # ç²å–è‚¡ç¥¨æ¸…å–®
        if specific_stocks:
            # å°‡å­—ç¬¦ä¸²åˆ—è¡¨è½‰æ›ç‚º(stock_id, name, market)æ ¼å¼
            stocks = []
            for stock_id in specific_stocks:
                # ç°¡å–®æ¨æ¸¬å¸‚å ´ï¼š4ä½æ•¸å­—ä¸”<=3000é€šå¸¸æ˜¯TWSEï¼Œå¦å‰‡æ˜¯TPEx
                market = 'twse' if stock_id.isdigit() and int(stock_id) <= 3000 else 'tpex'
                stocks.append((stock_id, stock_id, market))  # æš«æ™‚ç”¨stock_idç•¶ä½œname
            logger.info(f"ğŸ“Š ä½¿ç”¨æŒ‡å®šè‚¡ç¥¨æ¸…å–®: {len(stocks)} æª”")
        else:
            stocks = self.get_stock_list()
            if not stocks:
                logger.error("âŒ ç„¡æ³•ç²å–è‚¡ç¥¨æ¸…å–®")
                return
        
        if max_stocks and not specific_stocks:
            stocks = stocks[:max_stocks]
            logger.info(f"ğŸ“Š é™åˆ¶è™•ç†å‰ {max_stocks} æª”è‚¡ç¥¨")
        
        logger.info(f"ğŸ“Š ç¸½è¨ˆ {len(stocks)} æª”è‚¡ç¥¨éœ€è¦è™•ç†")
        
        # å»ºç«‹è¡¨çµæ§‹
        self.create_price_tables()
        
        # è™•ç†çµ±è¨ˆ
        processed = 0
        success = 0
        failed = 0
        
        start_time = datetime.now()
        
        for stock_id, name, market in stocks:
            logger.info(f"è™•ç† {processed+1}/{len(stocks)}: {stock_id} ({name}) - {market}")
            
            # è·³éå·²è¶³å¤ æ–°é®®çš„è‚¡ç¥¨ï¼ˆæ¸›å°‘é‡æŠ“ï¼‰
            if self.is_fresh_enough(stock_id, target_bars):
                logger.info(f"â†ªï¸ è·³é {stock_id}ï¼ˆè³‡æ–™å¤ æ–°ï¼‰")
                success += 1
                processed += 1
                continue
            
            if self.fetch_stock_historical_data(stock_id, market, target_bars):
                success += 1
            else:
                failed += 1
                
            processed += 1
            
            # å·²æœ‰å…¨åŸŸé€Ÿç‡é™åˆ¶ï¼Œç§»é™¤å›ºå®šå»¶é²
            
            # æ¯10æª”å ±å‘Šé€²åº¦ä¸¦é•·æ™‚é–“ä¼‘æ¯
            if processed % 10 == 0:
                elapsed = (datetime.now() - start_time).seconds
                eta = (elapsed / processed) * (len(stocks) - processed)
                logger.info(f"ğŸ“ˆ é€²åº¦: {processed}/{len(stocks)} (æˆåŠŸ:{success}, å¤±æ•—:{failed}), é ä¼°å‰©é¤˜: {eta/60:.1f}åˆ†é˜")
                # å·²æœ‰å…¨åŸŸé€Ÿç‡é™åˆ¶ï¼Œç§»é™¤å›ºå®šå»¶é²
        
        # å®Œæˆå ±å‘Š
        total_time = (datetime.now() - start_time).seconds
        logger.info(f"ğŸ‰ åƒ¹æ ¼æ•¸æ“šç®¡é“å®Œæˆ!")
        logger.info(f"ğŸ“Š è™•ç†çµæœ: ç¸½è¨ˆ{processed}æª”, æˆåŠŸ{success}æª”, å¤±æ•—{failed}æª”")
        logger.info(f"â±ï¸  ç¸½è€—æ™‚: {total_time/60:.1f}åˆ†é˜")
        
        # é©—è­‰çµæœ
        self.validate_price_data()
        
        return success, failed
    
    def validate_price_data(self):
        """é©—è­‰åƒ¹æ ¼æ•¸æ“šå®Œæ•´æ€§ï¼ˆå–®è¡¨ç‰ˆï¼‰"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æª¢æŸ¥ç¸½é«”æ•¸æ“š
            cursor.execute("SELECT COUNT(*), MIN(date), MAX(date) FROM daily_prices")
            total, min_date, max_date = cursor.fetchone() or (0, None, None)
            logger.info(f"âœ… daily_prices ç¸½ç­†æ•¸: {total}ï¼ˆ{min_date} ~ {max_date}ï¼‰")

            # å¸‚å ´åˆè¨ˆ
            for mkt in ("TWSE", "TPEx"):
                cursor.execute("""
                    SELECT COUNT(*), MIN(date), MAX(date)
                    FROM daily_prices WHERE market = ?
                """, (mkt,))
                c, dmin, dmax = cursor.fetchone() or (0, None, None)
                logger.info(f"ğŸ“Š {mkt}: {c} ç­†ï¼ˆ{dmin} ~ {dmax}ï¼‰")

            # æŠ½æ¨£ 5 æª”æª¢æŸ¥é€£çºŒæ€§
            cursor.execute("""
                SELECT stock_id, COUNT(*) as c FROM daily_prices
                GROUP BY stock_id ORDER BY c DESC LIMIT 5
            """)
            for sid, c in cursor.fetchall():
                cursor.execute("""
                    SELECT MIN(date), MAX(date) FROM daily_prices WHERE stock_id=?
                """, (sid,))
                dmin, dmax = cursor.fetchone()
                logger.info(f"ğŸ” {sid}: {c} ç­†ï¼ˆ{dmin} ~ {dmax}ï¼‰")
            
            conn.close()
            
        except Exception as e:
            logger.error(f"é©—è­‰åƒ¹æ ¼æ•¸æ“šå¤±æ•—: {e}")
    
    def update_existing_stocks(self, stock_list: List[str]):
        """
        æ›´æ–°ç¾æœ‰è‚¡ç¥¨åˆ°æœ€æ–°æ—¥æœŸ
        
        Args:
            stock_list: éœ€è¦æ›´æ–°çš„è‚¡ç¥¨æ¸…å–®
        """
        logger.info(f"ğŸ”„ æ›´æ–° {len(stock_list)} æª”ç¾æœ‰è‚¡ç¥¨åˆ°æœ€æ–°æ—¥æœŸ...")
        
        conn = sqlite3.connect(self.db_path)
        
        updated = 0
        failed = 0
        
        for i, stock_id in enumerate(stock_list):
            if i % 20 == 0:
                logger.info(f"æ›´æ–°é€²åº¦: {i}/{len(stock_list)} ({i/len(stock_list)*100:.1f}%)")
            
            try:
                # æª¢æŸ¥è©²è‚¡ç¥¨çš„æœ€æ–°æ—¥æœŸ
                latest_query = """
                SELECT MAX(date) as latest_date, COUNT(*) as total_bars
                FROM daily_prices 
                WHERE stock_id = ?
                """
                result = pd.read_sql_query(latest_query, conn, params=(stock_id,))
                
                if result.empty or result['latest_date'].iloc[0] is None:
                    logger.debug(f"{stock_id}: ç„¡ç¾æœ‰è³‡æ–™ï¼Œè·³éæ›´æ–°")
                    continue
                
                latest_date = result['latest_date'].iloc[0]
                
                # å¦‚æœè³‡æ–™å·²ç¶“æ˜¯æœ€æ–°çš„ï¼Œè·³é
                today = datetime.now().strftime('%Y-%m-%d')
                if latest_date >= today:
                    continue
                
                # å˜—è©¦ç²å–æœ€æ–°2-3å¤©çš„æ•¸æ“š
                success = False
                for market in ['twse', 'tpex']:
                    try:
                        # ä½¿ç”¨ç¾æœ‰APIç²å–æœ€æ–°å¹¾å¤©çš„æ•¸æ“š
                        recent_data = self.get_recent_trading_data(stock_id, market, days=5)
                        if recent_data:
                            self.store_daily_prices(stock_id, recent_data, market, 'recent_update')
                            success = True
                            break
                    except:
                        continue
                
                if success:
                    updated += 1
                else:
                    failed += 1
                
                # æ§åˆ¶è«‹æ±‚é »ç‡
                time.sleep(0.2)
                
            except Exception as e:
                failed += 1
                logger.error(f"{stock_id}: æ›´æ–°éŒ¯èª¤ - {e}")
        
        conn.close()
        logger.info(f"ğŸ“Š æ›´æ–°å®Œæˆï¼æˆåŠŸ: {updated}, å¤±æ•—: {failed}")
        return updated, failed
    
    def get_recent_trading_data(self, stock_id: str, market: str, days: int = 5):
        """ç²å–æœ€è¿‘å¹¾å¤©çš„äº¤æ˜“æ•¸æ“š"""
        try:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            
            return self.fetch_stock_data_twse_api(
                stock_id, market, 
                start_date.strftime('%Y-%m-%d'),
                end_date.strftime('%Y-%m-%d')
            )
        except Exception as e:
            logger.debug(f"ç²å– {stock_id} æœ€æ–°æ•¸æ“šå¤±æ•—: {e}")
            return None

def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å°è‚¡æ­·å²åƒ¹æ ¼æ•¸æ“šç²å–")
    parser.add_argument("--max-stocks", type=int, help="æœ€å¤§è™•ç†è‚¡ç¥¨æ•¸é‡ (æ¸¬è©¦ç”¨)")
    parser.add_argument("--bars", type=int, default=60, help="ç›®æ¨™Kç·šæ ¹æ•¸ (é è¨­60æ ¹)")
    parser.add_argument("--db-path", default="data/cleaned/taiwan_stocks_cleaned.db", help="è³‡æ–™åº«è·¯å¾‘")
    
    args = parser.parse_args()
    
    pipeline = TaiwanStockPriceDataPipeline(args.db_path)
    pipeline.run_price_data_pipeline(args.max_stocks, args.bars)

if __name__ == "__main__":
    main()