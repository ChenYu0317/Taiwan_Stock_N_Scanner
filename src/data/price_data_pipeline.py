#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°è‚¡æ­·å²åƒ¹æ ¼æ•¸æ“šç²å–ç®¡é“
ä¿®å¾©Phase 1éºæ¼ï¼šå¾å®˜æ–¹ä¾†æºç²å–çœŸå¯¦çš„æ­·å²OHLCæ•¸æ“š
"""

import requests
import pandas as pd
import sqlite3
import time
import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
import os
from urllib.parse import urlencode
import calendar
from dateutil.relativedelta import relativedelta
from dateutil import tz
import math
import numpy as np
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import re

# å°åŒ—æ™‚å€å®šç¾©
TAIPEI = tz.gettz("Asia/Taipei")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def sanitize_stock_id(stock_id: str) -> str:
    """æ ¡é©—è‚¡ç¥¨ä»£ç¢¼æ ¼å¼"""
    if not re.fullmatch(r"\d{4}", str(stock_id)):
        raise ValueError(f"invalid stock_id: {stock_id}")
    return stock_id

def ensure_daily_prices_table(conn):
    """å»ºç«‹å–®ä¸€åƒ¹æ ¼äº‹å¯¦è¡¨"""
    conn.execute("""
    CREATE TABLE IF NOT EXISTS daily_prices (
        stock_id TEXT NOT NULL,
        date DATE NOT NULL,
        open REAL, high REAL, low REAL, close REAL, volume INTEGER,
        market TEXT, source TEXT,
        ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        PRIMARY KEY (stock_id, date)
    )
    """)
    conn.execute("CREATE INDEX IF NOT EXISTS idx_prices_date ON daily_prices(date)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_prices_market ON daily_prices(market)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_prices_stock_date ON daily_prices(stock_id, date)")
    conn.commit()

class TaiwanStockPriceDataPipeline:
    """å°è‚¡æ­·å²åƒ¹æ ¼æ•¸æ“šç®¡é“"""
    
    def __init__(self, db_path: str = "data/cleaned/taiwan_stocks_cleaned.db"):
        self.db_path = db_path
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        # åŠ å…¥é‡è©¦æ©Ÿåˆ¶
        retry = Retry(total=3, backoff_factor=0.6, status_forcelist=[429, 500, 502, 503, 504])
        self.session.mount("https://", HTTPAdapter(max_retries=retry))

    def is_fresh_enough(self, stock_id: str, target_bars: int, freshness_days: int = 7) -> bool:
        """æª¢æŸ¥è‚¡ç¥¨æ•¸æ“šæ˜¯å¦è¶³å¤ æ–°é®®ï¼Œé¿å…é‡è¤‡æŠ“å–"""
        conn = sqlite3.connect(self.db_path)
        try:
            result = conn.execute(
                "SELECT COUNT(*), MAX(date) FROM daily_prices WHERE stock_id=?",
                (stock_id,)
            ).fetchone()
            c, maxd = result or (0, None)
        finally:
            conn.close()
            
        if not maxd:
            return False
            
        maxd = pd.to_datetime(maxd)
        today = pd.Timestamp.now(tz=TAIPEI).normalize().tz_localize(None)
        
        if c >= target_bars and (today - maxd).days <= freshness_days:
            return True
        return False
        
    def get_stock_list(self) -> List[Tuple[str, str, str]]:
        """å¾è³‡æ–™åº«ç²å–è‚¡ç¥¨æ¸…å–®"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT stock_id, name, market FROM stock_universe WHERE status='active' ORDER BY stock_id")
            stocks = cursor.fetchall()
            conn.close()
            return stocks
        except Exception as e:
            logger.error(f"ç²å–è‚¡ç¥¨æ¸…å–®å¤±æ•—: {e}")
            return []
    
    def fetch_twse_stock_data(self, stock_id: str, year: int, month: int) -> Optional[pd.DataFrame]:
        """
        å¾TWSEç²å–å–®ä¸€è‚¡ç¥¨çš„æœˆåº¦æ­·å²è³‡æ–™
        
        Args:
            stock_id: è‚¡ç¥¨ä»£ç¢¼
            year: å¹´ä»½
            month: æœˆä»½
            
        Returns:
            DataFrame æˆ– None
        """
        try:
            # TWSE STOCK_DAY API
            url = "https://www.twse.com.tw/exchangeReport/STOCK_DAY"
            params = {
                'response': 'json',
                'date': f"{year}{month:02d}01",
                'stockNo': stock_id
            }
            
            response = self.session.get(url, params=params, timeout=15)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get('stat') != 'OK':
                logger.warning(f"TWSE APIè¿”å›éOKç‹€æ…‹: {stock_id} {year}/{month}")
                return None
                
            if not data.get('data'):
                logger.debug(f"ç„¡æ•¸æ“š: {stock_id} {year}/{month}")
                return None
            
            # è§£ææ•¸æ“š
            df = pd.DataFrame(data['data'])
            if len(df) == 0:
                return None
            
            # TWSE æ¬„ä½æ”¹ç‰ˆéŸŒæ€§ (å¿…è£œé …ç›®3)
            fields = data.get('fields') or []
            if fields:
                # ä½¿ç”¨ API æä¾›çš„æ¬„ä½åç¨±é€²è¡Œæ˜ å°„
                df.columns = fields
                
                # å»ºç«‹æ¬„åæ˜ å°„ï¼ˆæ”¯æ´ä¸­è‹±æ–‡ï¼‰
                field_mapping = {}
                for i, field in enumerate(fields):
                    if field in ['æ—¥æœŸ', 'Date']:
                        field_mapping['date'] = i
                    elif field in ['æˆäº¤è‚¡æ•¸', 'æˆäº¤é‡', 'Volume']:
                        field_mapping['volume'] = i
                    elif field in ['é–‹ç›¤åƒ¹', 'é–‹ç›¤', 'Open']:
                        field_mapping['open'] = i
                    elif field in ['æœ€é«˜åƒ¹', 'æœ€é«˜', 'High']:
                        field_mapping['high'] = i
                    elif field in ['æœ€ä½åƒ¹', 'æœ€ä½', 'Low']:
                        field_mapping['low'] = i
                    elif field in ['æ”¶ç›¤åƒ¹', 'æ”¶ç›¤', 'Close']:
                        field_mapping['close'] = i
                
                # ç¢ºèªå¿…è¦æ¬„ä½éƒ½å­˜åœ¨
                required_fields = ['date', 'volume', 'open', 'high', 'low', 'close']
                if all(field in field_mapping for field in required_fields):
                    # é‡æ–°æ’åºæ¬„ä½
                    df = df.iloc[:, [field_mapping[field] for field in required_fields]]
                    df.columns = required_fields
                else:
                    logger.warning(f"TWSE API æ¬„ä½çµæ§‹è®Šæ›´ï¼Œå›é€€åˆ°å›ºå®šæ ¼å¼: {fields}")
                    # å›é€€åˆ°å›ºå®šæ¬„ä½é †åº
                    df.columns = ['date', 'volume', 'turnover', 'open', 'high', 'low', 'close', 'change', 'transaction']
            else:
                # Fallback: ä½¿ç”¨å›ºå®šæ¬„ä½åç¨± (TWSEæ ¼å¼)
                df.columns = ['date', 'volume', 'turnover', 'open', 'high', 'low', 'close', 'change', 'transaction']
            
            # æ•¸æ“šæ¸…ç† - è™•ç†æ°‘åœ‹å¹´è½‰è¥¿å…ƒå¹´
            def convert_roc_date(date_str):
                """è½‰æ›æ°‘åœ‹å¹´æ—¥æœŸç‚ºè¥¿å…ƒå¹´"""
                parts = date_str.split('/')
                if len(parts) == 3:
                    roc_year = int(parts[0])
                    month = parts[1]
                    day = parts[2]
                    ad_year = roc_year + 1911  # æ°‘åœ‹å¹´è½‰è¥¿å…ƒå¹´
                    return f"{ad_year}-{month.zfill(2)}-{day.zfill(2)}"
                return date_str
            
            df['date'] = df['date'].apply(convert_roc_date)
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
            
            # è½‰æ›æ•¸å­—æ¬„ä½ (æ›´å®‰å…¨çš„æ•¸å€¼æ¸…æ´—ï¼Œä¿ç•™åŸå§‹å€¼)
            numeric_cols = ['volume', 'turnover', 'open', 'high', 'low', 'close']
            for col in numeric_cols:
                # ä¿ç•™åŸå§‹å€¼ç”¨æ–¼è¿½æº¯
                df[f'{col}_raw'] = df[col].copy()
                
                # æ¸…æ´—ä¸¦è½‰æ›
                s = df[col].astype(str)
                s = s.str.replace(",", "", regex=False)
                s = s.replace({"--": None, "â€”": None, "": None, "nan": None})
                df[col] = pd.to_numeric(s, errors="coerce")
            
            # éæ¿¾ç„¡æ•ˆæ•¸æ“š
            df = df.dropna(subset=['open', 'high', 'low', 'close'])
            df = df[df['close'] > 0]
            
            if len(df) == 0:
                return None
                
            # åªä¿ç•™éœ€è¦çš„æ¬„ä½ä¸¦æ·»åŠ ä¾†æºæ¨™è¨˜
            result = df[['date', 'open', 'high', 'low', 'close', 'volume']].copy()
            result['stock_id'] = stock_id
            result['market'] = 'TWSE'
            result['source'] = 'TWSE_STOCK_DAY'
            
            return result
            
        except Exception as e:
            logger.error(f"ç²å–TWSEæ•¸æ“šå¤±æ•— {stock_id} {year}/{month}: {e}")
            return None
    
    def fetch_tpex_stock_data(self, stock_id: str, year: int, month: int) -> Optional[pd.DataFrame]:
        """
        å¾TPExç²å–å–®ä¸€è‚¡ç¥¨çš„æœˆåº¦æ­·å²è³‡æ–™
        
        Args:
            stock_id: è‚¡ç¥¨ä»£ç¢¼
            year: å¹´ä»½ 
            month: æœˆä»½
            
        Returns:
            DataFrame æˆ– None
        """
        try:
            # TPEx å€‹è‚¡æ—¥æˆäº¤è³‡è¨Š
            url = "https://www.tpex.org.tw/web/stock/aftertrading/daily_close_quotes/stk_quote_result.php"
            
            # è½‰æ›ç‚ºæ°‘åœ‹å¹´
            roc_year = year - 1911
            
            params = {
                'l': 'zh-tw',
                'o': 'json',
                'd': f"{roc_year}/{month:02d}",  # æ°‘åœ‹å¹´æ ¼å¼
                'stkno': stock_id
            }
            
            response = self.session.get(url, params=params, timeout=15)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get('stat') != 'OK':
                logger.warning(f"TPEx JSON APIè¿”å›éOKç‹€æ…‹: {stock_id} {year}/{month}ï¼Œå˜—è©¦CSVå‚™æ´")
                # å˜—è©¦CSVå‚™æ´
                return self.fetch_tpex_stock_data_csv_fallback(stock_id, year, month)
                
            if not data.get('aaData'):
                logger.debug(f"ç„¡æ•¸æ“š: {stock_id} {year}/{month}")
                return None
            
            # è§£ææ•¸æ“š
            rows = []
            for row in data['aaData']:
                if len(row) >= 6:
                    # TPExæ ¼å¼: [æ—¥æœŸ, é–‹ç›¤, æœ€é«˜, æœ€ä½, æ”¶ç›¤, æˆäº¤é‡, ...]
                    date_str = f"{year}-{row[0].replace('/', '-')}"
                    rows.append({
                        'date': date_str,
                        'open': row[1],
                        'high': row[2], 
                        'low': row[3],
                        'close': row[4],
                        'volume': row[5] if len(row) > 5 else 0
                    })
            
            if not rows:
                return None
                
            df = pd.DataFrame(rows)
            
            # æ•¸æ“šæ¸…ç†
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
            
            # è½‰æ›æ•¸å­—æ¬„ä½ (æ›´å®‰å…¨çš„æ•¸å€¼æ¸…æ´—ï¼Œä¿ç•™åŸå§‹å€¼)
            numeric_cols = ['open', 'high', 'low', 'close', 'volume']
            for col in numeric_cols:
                # ä¿ç•™åŸå§‹å€¼ç”¨æ–¼è¿½æº¯
                df[f'{col}_raw'] = df[col].copy()
                
                # æ¸…æ´—ä¸¦è½‰æ›
                s = df[col].astype(str)
                s = s.str.replace(",", "", regex=False)
                s = s.replace({"--": None, "â€”": None, "": None, "nan": None})
                df[col] = pd.to_numeric(s, errors="coerce")
            
            # éæ¿¾ç„¡æ•ˆæ•¸æ“š
            df = df.dropna(subset=['date', 'open', 'high', 'low', 'close'])
            df = df[df['close'] > 0]
            
            if len(df) == 0:
                return None
                
            df['stock_id'] = stock_id
            df['market'] = 'TPEx'
            df['source'] = 'TPEX_JSON'
            
            return df[['date', 'open', 'high', 'low', 'close', 'volume', 'stock_id', 'market', 'source']]
            
        except Exception as e:
            logger.error(f"ç²å–TPExæ•¸æ“šå¤±æ•— {stock_id} {year}/{month}: {e}")
            # å˜—è©¦CSVå‚™æ´
            return self.fetch_tpex_stock_data_csv_fallback(stock_id, year, month)

    def fetch_tpex_stock_data_csv_fallback(self, stock_id: str, year: int, month: int) -> Optional[pd.DataFrame]:
        """TPEx CSV å‚™æ´æ©Ÿåˆ¶ (æ°‘åœ‹å¹´æ ¼å¼)"""
        try:
            # è½‰æ›ç‚ºæ°‘åœ‹å¹´
            roc_year = year - 1911
            
            # TPEx CSV ä¸‹è¼‰ URL (æ°‘åœ‹å¹´æ ¼å¼)
            url = "https://www.tpex.org.tw/web/stock/historical_trading/stk_quote_download.php"
            params = {
                "l": "zh-tw",
                "d": f"{roc_year}/{month:02d}",
                "stkno": stock_id,
                "download": "csv"
            }
            
            logger.info(f"å˜—è©¦ TPEx CSV å‚™æ´: {stock_id} {roc_year}/{month:02d}")
            
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºCSVå…§å®¹
            content_type = response.headers.get('content-type', '')
            if 'text/csv' not in content_type and 'application/csv' not in content_type:
                logger.warning(f"TPEx CSV å›æ‡‰é¡å‹ç•°å¸¸: {content_type}")
            
            # è§£æ CSV å…§å®¹
            lines = response.text.strip().split('\n')
            if len(lines) < 2:
                logger.warning(f"TPEx CSV ç„¡è³‡æ–™: {stock_id} {year}-{month:02d}")
                return None
            
            rows = []
            header = lines[0].split(',')
            
            # æ›´éŸŒæ€§çš„ TPEx CSV æ¬„ä½åæŠ“å–
            def _find_col(cols, keys):
                """ä»¥åŒ…å«å¼æ¯”å°æ‰¾æ¬„ä½åï¼Œæ›´éŸŒæ€§è™•ç†å¾®èª¿"""
                for k in keys:
                    for c in cols:
                        if k in c:
                            return c
                return None
            
            try:
                # ä½¿ç”¨åŒ…å«å¼åŒ¹é…æ‰¾æ¬„ä½å (è¿”å›æ¬„ä½åè€Œéç´¢å¼•)
                date_col = _find_col(header, ['æ—¥æœŸ', 'æˆäº¤æ—¥æœŸ'])
                open_col = _find_col(header, ['é–‹ç›¤'])
                high_col = _find_col(header, ['æœ€é«˜'])
                low_col = _find_col(header, ['æœ€ä½'])
                close_col = _find_col(header, ['æ”¶ç›¤'])
                vol_col = _find_col(header, ['æˆäº¤è‚¡æ•¸', 'æˆäº¤è‚¡æ•¸(åƒè‚¡)'])
                
                # è½‰æ›ç‚ºç´¢å¼•
                date_idx = header.index(date_col) if date_col else None
                open_idx = header.index(open_col) if open_col else None
                high_idx = header.index(high_col) if high_col else None
                low_idx = header.index(low_col) if low_col else None
                close_idx = header.index(close_col) if close_col else None
                volume_idx = header.index(vol_col) if vol_col else None
                        
                if None in [date_idx, open_idx, high_idx, low_idx, close_idx, volume_idx]:
                    logger.error(f"TPEx CSV æ‰¾ä¸åˆ°å¿…è¦æ¬„ä½: {header}")
                    logger.error(f"åŒ¹é…çµæœ: date={date_idx}, open={open_idx}, high={high_idx}, low={low_idx}, close={close_idx}, volume={volume_idx}")
                    return None
                    
            except Exception as e:
                logger.error(f"TPEx CSV æ¬„ä½è§£æç•°å¸¸: {e}")
                return None
            
            for line in lines[1:]:
                if not line.strip():
                    continue
                    
                fields = line.split(',')
                if len(fields) != len(header):
                    continue
                
                try:
                    # è§£ææ—¥æœŸ (æ°‘åœ‹å¹´ xxx/mm/dd æ ¼å¼)
                    date_str = fields[date_idx].strip()
                    if '/' not in date_str:
                        continue
                    
                    roc_parts = date_str.split('/')
                    if len(roc_parts) != 3:
                        continue
                    
                    roc_year_str, month_str, day_str = roc_parts
                    ad_year = int(roc_year_str) + 1911
                    date_obj = datetime(ad_year, int(month_str), int(day_str))
                    
                    # è§£æåƒ¹æ ¼ (å»é™¤åƒåˆ†ä½é€—è™Ÿ)
                    def parse_price(value_str):
                        cleaned = value_str.replace(',', '').replace('--', '').replace('â€”', '').strip()
                        return float(cleaned) if cleaned and cleaned != '0' else None
                    
                    open_price = parse_price(fields[open_idx])
                    high_price = parse_price(fields[high_idx])
                    low_price = parse_price(fields[low_idx])
                    close_price = parse_price(fields[close_idx])
                    
                    # æˆäº¤é‡ (åƒè‚¡è½‰ç‚ºè‚¡)
                    volume_str = fields[volume_idx].replace(',', '').strip()
                    volume = int(float(volume_str) * 1000) if volume_str and volume_str not in ['--', 'â€”'] else 0
                    
                    # åŸºæœ¬è³‡æ–™é©—è­‰
                    if not all([open_price, high_price, low_price, close_price]):
                        continue
                    if high_price < max(open_price, close_price):
                        continue
                    if low_price > min(open_price, close_price):
                        continue
                    
                    rows.append({
                        'date': date_obj,
                        'open': open_price,
                        'high': high_price,
                        'low': low_price,
                        'close': close_price,
                        'volume': volume
                    })
                    
                except (ValueError, IndexError) as e:
                    logger.debug(f"TPEx CSV è§£æå¤±æ•—è¡Œ: {line[:50]}, error: {e}")
                    continue
            
            if not rows:
                logger.warning(f"TPEx CSV å‚™æ´ç„¡æœ‰æ•ˆè³‡æ–™: {stock_id} {year}-{month:02d}")
                return None
                
            df = pd.DataFrame(rows)
            df['stock_id'] = stock_id
            df['market'] = 'TPEx'
            df['source'] = 'TPEX_CSV_BACKUP'
            
            logger.info(f"TPEx CSV å‚™æ´æˆåŠŸ: {stock_id} {year}-{month:02d}, å–å¾— {len(rows)} ç­†")
            return df[['date', 'open', 'high', 'low', 'close', 'volume', 'stock_id', 'market', 'source']]
            
        except Exception as e:
            logger.error(f"TPEx CSV å‚™æ´å¤±æ•— {stock_id} {year}-{month:02d}: {e}")
            return None
    
    def create_price_tables(self):
        """å»ºç«‹åƒ¹æ ¼æ•¸æ“šè¡¨çµæ§‹"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æª¢æŸ¥å·²å­˜åœ¨çš„åƒ¹æ ¼è¡¨
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name LIKE 'stock_%'")
            existing_tables = [row[0] for row in cursor.fetchall()]
            
            logger.info(f"ç¾æœ‰åƒ¹æ ¼è¡¨æ•¸é‡: {len(existing_tables)}")
            conn.close()
            
        except Exception as e:
            logger.error(f"æª¢æŸ¥åƒ¹æ ¼è¡¨å¤±æ•—: {e}")
    
    def save_stock_price_data(self, stock_id: str, market: str, source: str, df: pd.DataFrame):
        """ä¿å­˜è‚¡ç¥¨åƒ¹æ ¼è³‡æ–™åˆ°å–®ä¸€äº‹å¯¦è¡¨ (æœå‹™ç´šæ¶æ§‹)"""
        conn = None
        try:
            # æ ¡é©—è‚¡ç¥¨ä»£ç¢¼
            stock_id = sanitize_stock_id(stock_id)
            
            conn = sqlite3.connect(self.db_path)
            # å„ªåŒ–SQLiteæ•ˆèƒ½
            conn.execute("PRAGMA journal_mode=WAL;")
            conn.execute("PRAGMA synchronous=NORMAL;")
            
            # ç¢ºä¿å–®è¡¨æ¶æ§‹å­˜åœ¨
            ensure_daily_prices_table(conn)
            
            # æ‰¹æ¬¡æ’å…¥æ•¸æ“š - ä½¿ç”¨ per-row ç²¾æº–ä¾†æºè¿½æº¯ (å¿…è£œé …ç›®2)
            rows = []
            for r in df.itertuples(index=False):
                try:
                    # ç¢ºä¿æ‰€æœ‰æ•¸å€¼éƒ½æ˜¯æœ‰æ•ˆçš„
                    if pd.notna(r.open) and pd.notna(r.high) and pd.notna(r.low) and pd.notna(r.close) and pd.notna(r.volume):
                        # å„ªå…ˆä½¿ç”¨ df å…§çš„ market/sourceï¼Œå›é€€åˆ°åƒæ•¸å‚³å…¥å€¼
                        row_market = getattr(r, 'market', market)
                        row_source = getattr(r, 'source', source)
                        
                        rows.append((
                            stock_id,
                            r.date.strftime("%Y-%m-%d"), 
                            float(r.open), 
                            float(r.high),
                            float(r.low), 
                            float(r.close), 
                            int(r.volume),
                            row_market,
                            row_source
                        ))
                except (ValueError, AttributeError, OverflowError) as e:
                    logger.warning(f"è·³éç„¡æ•ˆè¨˜éŒ„ {stock_id} {r.date}: {e}")
                    continue
            
            if rows:
                # å¯«å…¥æ•ˆèƒ½å¾®èª¿ (æ‡‰åŠ é …ç›®7)
                # æ‰¹æ¬¡å¯«å…¥ä»¥é™ä½ I/O é »ç‡
                batch_size = 1000
                for i in range(0, len(rows), batch_size):
                    batch = rows[i:i + batch_size]
                    conn.executemany("""
                        INSERT OR REPLACE INTO daily_prices
                        (stock_id, date, open, high, low, close, volume, market, source)
                        VALUES (?,?,?,?,?,?,?,?,?)
                    """, batch)
                    
                    # æ¯æ‰¹æ¬¡å¾ŒçŸ­æš«ä¼‘æ¯ï¼Œé¿å… I/O å³°å€¼
                    if i + batch_size < len(rows):
                        time.sleep(0.01)
                
                conn.commit()
                logger.debug(f"ä¿å­˜ {stock_id}: {len(rows)} ç­†è¨˜éŒ„åˆ°å–®è¡¨")
            else:
                logger.warning(f"è‚¡ç¥¨ {stock_id} ç„¡æœ‰æ•ˆè¨˜éŒ„å¯ä¿å­˜")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ {stock_id} åƒ¹æ ¼æ•¸æ“šå¤±æ•—: {e}")
        finally:
            if conn:
                conn.close()
    
    def fetch_stock_historical_data(self, stock_id: str, market: str, target_bars: int = 60) -> bool:
        """
        ç²å–è‚¡ç¥¨æ­·å²è³‡æ–™ (ç²¾ç¢ºæ§åˆ¶Kç·šæ ¹æ•¸) - ä¿®æ­£ç‰ˆ
        
        Args:
            stock_id: è‚¡ç¥¨ä»£ç¢¼
            market: å¸‚å ´ (TWSE/TPEx)
            target_bars: ç›®æ¨™Kç·šæ ¹æ•¸ (é è¨­60æ ¹)
            
        Returns:
            æˆåŠŸèˆ‡å¦
        """
        all_data = []
        
        try:
            current_date = datetime.now()
            # å‹•æ…‹è¨ˆç®—éœ€è¦æœˆæ•¸ï¼šä¸€å€‹æœˆç´„18äº¤æ˜“æ—¥
            need_months = max(3, math.ceil(target_bars / 18) + 1)
            months_tried = 0
            total_records = 0
            
            # ç¬¬ä¸€è¼ªï¼šæŒ‰é ä¼°æœˆæ•¸æŠ“å–
            while total_records < target_bars and months_tried < need_months:
                year = current_date.year
                month = current_date.month
                months_tried += 1
                
                logger.debug(f"ç²å– {stock_id} {year}/{month} (ç›®æ¨™: {target_bars}æ ¹, å·²ç²å–: {len(all_data)}æ ¹)")
                
                # æ ¹æ“šå¸‚å ´é¸æ“‡API
                if market == 'TWSE':
                    df = self.fetch_twse_stock_data(stock_id, year, month)
                else:  # TPEx
                    df = self.fetch_tpex_stock_data(stock_id, year, month)
                
                if df is not None and len(df) > 0:
                    all_data.append(df)
                    total_records += len(df)
                    logger.info(f"âœ… {stock_id} {year}/{month}: {len(df)} ç­† (ç´¯è¨ˆ: {total_records})")
                else:
                    logger.warning(f"âŒ {stock_id} {year}/{month}: ç„¡æ•¸æ“š")
                
                # é—œéµä¿®æ­£ï¼šå¾€å‰ç§»å‹•ä¸€å€‹æœˆ
                current_date = current_date - relativedelta(months=1)
                
                # é¿å…éæ–¼é »ç¹è«‹æ±‚
                time.sleep(0.5)
            
            # ç¬¬äºŒè¼ªï¼šå¦‚æœé‚„ä¸å¤ ï¼Œå†æœ€å¤šæŠ“6å€‹æœˆä½œç‚ºä¿éšª
            max_extra_months = 6
            while total_records < target_bars and months_tried < need_months + max_extra_months:
                year = current_date.year
                month = current_date.month
                months_tried += 1
                
                logger.debug(f"ä¿éšªè¼ª: ç²å– {stock_id} {year}/{month}")
                
                if market == 'TWSE':
                    df = self.fetch_twse_stock_data(stock_id, year, month)
                else:
                    df = self.fetch_tpex_stock_data(stock_id, year, month)
                
                if df is not None and len(df) > 0:
                    all_data.append(df)
                    total_records += len(df)
                    logger.info(f"âœ… {stock_id} {year}/{month}: {len(df)} ç­† (ä¿éšªè¼ªç´¯è¨ˆ: {total_records})")
                
                current_date = current_date - relativedelta(months=1)
                time.sleep(0.5)
            
            if not all_data:
                logger.warning(f"âŒ è‚¡ç¥¨ {stock_id} ç„¡æ³•ç²å–ä»»ä½•æ•¸æ“š")
                return False
            
            # åˆä½µæ‰€æœ‰DataFrame
            df_all = pd.concat(all_data, ignore_index=True)
            df_all['date'] = pd.to_datetime(df_all['date'])
            
            # å»é‡èˆ‡æ’åº
            df_all = df_all.dropna(subset=['date', 'open', 'high', 'low', 'close'])
            
            # é¿å…æœªä¾†æ—¥èª¤å…¥åº« - å°åŒ—æ™‚å€ä¸€è‡´åŒ– (å¿…è£œé …ç›®1)
            today = pd.Timestamp.now(tz=TAIPEI).normalize().tz_localize(None)
            future_dates = df_all[df_all['date'] > today]
            if len(future_dates) > 0:
                logger.warning(f"âš ï¸ {stock_id} ç™¼ç¾ {len(future_dates)} ç­†æœªä¾†æ—¥æœŸï¼Œå·²éæ¿¾: {future_dates['date'].tolist()}")
                df_all = df_all[df_all['date'] <= today]
            
            df_all = df_all.drop_duplicates(subset=['date']).sort_values('date').reset_index(drop=True)
            
            # ç•°å¸¸é‡åƒ¹ä¿å®ˆè™•ç† (æ‡‰åŠ é …ç›®4)
            df_all = df_all.dropna(subset=['open', 'high', 'low', 'close'])
            df_all = df_all[df_all['close'] > 0]
            # è‹¥ä¸æƒ³æŠŠåœç‰Œæ—¥å¯«å…¥ï¼Œå–æ¶ˆä¸‹è¡Œæ³¨é‡‹ï¼š
            # df_all = df_all[df_all['volume'].fillna(0) > 0]
            
            # æŠ“ä¸æ»¿çš„æ˜ç¢ºå‘Šè­¦ (æ‡‰åŠ é …ç›®5)
            if len(df_all) < target_bars:
                logger.warning(f"âš ï¸ {stock_id} åƒ… {len(df_all)}/{target_bars} æ ¹ï¼ˆmonths_tried={months_tried}ï¼‰")
            
            if len(df_all) > target_bars:
                df_all = df_all.tail(target_bars).reset_index(drop=True)
            
            # ä¿å­˜åˆ°è³‡æ–™åº« (å–®è¡¨æ¶æ§‹)
            source = f"{market}_API"
            self.save_stock_price_data(stock_id, market, source, df_all)
            
            # ä¿®æ­£æ—¥å¿—æ ¼å¼
            start = df_all.iloc[0]['date'].strftime('%Y-%m-%d')
            end = df_all.iloc[-1]['date'].strftime('%Y-%m-%d')
            logger.info(f"âœ… è‚¡ç¥¨ {stock_id} å®Œæˆ: å…± {len(df_all)} ç­† ({start} ~ {end})")
            return True
        
        except Exception as e:
            logger.error(f"ç²å– {stock_id} æ­·å²è³‡æ–™å¤±æ•—: {e}")
            return False
    
    def run_price_data_pipeline(self, max_stocks: Optional[int] = None, target_bars: int = 60):
        """
        åŸ·è¡Œåƒ¹æ ¼æ•¸æ“šç®¡é“
        
        Args:
            max_stocks: æœ€å¤§è™•ç†è‚¡ç¥¨æ•¸ (None = å…¨éƒ¨)
            target_bars: ç›®æ¨™Kç·šæ ¹æ•¸ (é è¨­60æ ¹)
        """
        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œåƒ¹æ ¼æ•¸æ“šç®¡é“...")
        
        # ç²å–è‚¡ç¥¨æ¸…å–®
        stocks = self.get_stock_list()
        if not stocks:
            logger.error("âŒ ç„¡æ³•ç²å–è‚¡ç¥¨æ¸…å–®")
            return
        
        if max_stocks:
            stocks = stocks[:max_stocks]
            logger.info(f"ğŸ“Š é™åˆ¶è™•ç†å‰ {max_stocks} æª”è‚¡ç¥¨")
        
        logger.info(f"ğŸ“Š ç¸½è¨ˆ {len(stocks)} æª”è‚¡ç¥¨éœ€è¦è™•ç†")
        
        # å»ºç«‹è¡¨çµæ§‹
        self.create_price_tables()
        
        # è™•ç†çµ±è¨ˆ
        processed = 0
        success = 0
        failed = 0
        
        start_time = datetime.now()
        
        for stock_id, name, market in stocks:
            logger.info(f"è™•ç† {processed+1}/{len(stocks)}: {stock_id} ({name}) - {market}")
            
            # è·³éå·²è¶³å¤ æ–°é®®çš„è‚¡ç¥¨ï¼ˆæ¸›å°‘é‡æŠ“ï¼‰
            if self.is_fresh_enough(stock_id, target_bars):
                logger.info(f"â†ªï¸ è·³é {stock_id}ï¼ˆè³‡æ–™å¤ æ–°ï¼‰")
                success += 1
                processed += 1
                continue
            
            if self.fetch_stock_historical_data(stock_id, market, target_bars):
                success += 1
            else:
                failed += 1
                
            processed += 1
            
            # æ¯è™•ç†1æª”å°±ä¼‘æ¯ä¸€ä¸‹ï¼Œé¿å…è¢«APIå°é–
            time.sleep(1.0)
            
            # æ¯10æª”å ±å‘Šé€²åº¦ä¸¦é•·æ™‚é–“ä¼‘æ¯
            if processed % 10 == 0:
                elapsed = (datetime.now() - start_time).seconds
                eta = (elapsed / processed) * (len(stocks) - processed)
                logger.info(f"ğŸ“ˆ é€²åº¦: {processed}/{len(stocks)} (æˆåŠŸ:{success}, å¤±æ•—:{failed}), é ä¼°å‰©é¤˜: {eta/60:.1f}åˆ†é˜")
                # æ¯10æª”ä¼‘æ¯5ç§’ï¼Œé¿å…éåº¦è«‹æ±‚
                time.sleep(5.0)
        
        # å®Œæˆå ±å‘Š
        total_time = (datetime.now() - start_time).seconds
        logger.info(f"ğŸ‰ åƒ¹æ ¼æ•¸æ“šç®¡é“å®Œæˆ!")
        logger.info(f"ğŸ“Š è™•ç†çµæœ: ç¸½è¨ˆ{processed}æª”, æˆåŠŸ{success}æª”, å¤±æ•—{failed}æª”")
        logger.info(f"â±ï¸  ç¸½è€—æ™‚: {total_time/60:.1f}åˆ†é˜")
        
        # é©—è­‰çµæœ
        self.validate_price_data()
    
    def validate_price_data(self):
        """é©—è­‰åƒ¹æ ¼æ•¸æ“šå®Œæ•´æ€§ï¼ˆå–®è¡¨ç‰ˆï¼‰"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æª¢æŸ¥ç¸½é«”æ•¸æ“š
            cursor.execute("SELECT COUNT(*), MIN(date), MAX(date) FROM daily_prices")
            total, min_date, max_date = cursor.fetchone() or (0, None, None)
            logger.info(f"âœ… daily_prices ç¸½ç­†æ•¸: {total}ï¼ˆ{min_date} ~ {max_date}ï¼‰")

            # å¸‚å ´åˆè¨ˆ
            for mkt in ("TWSE", "TPEx"):
                cursor.execute("""
                    SELECT COUNT(*), MIN(date), MAX(date)
                    FROM daily_prices WHERE market = ?
                """, (mkt,))
                c, dmin, dmax = cursor.fetchone() or (0, None, None)
                logger.info(f"ğŸ“Š {mkt}: {c} ç­†ï¼ˆ{dmin} ~ {dmax}ï¼‰")

            # æŠ½æ¨£ 5 æª”æª¢æŸ¥é€£çºŒæ€§
            cursor.execute("""
                SELECT stock_id, COUNT(*) as c FROM daily_prices
                GROUP BY stock_id ORDER BY c DESC LIMIT 5
            """)
            for sid, c in cursor.fetchall():
                cursor.execute("""
                    SELECT MIN(date), MAX(date) FROM daily_prices WHERE stock_id=?
                """, (sid,))
                dmin, dmax = cursor.fetchone()
                logger.info(f"ğŸ” {sid}: {c} ç­†ï¼ˆ{dmin} ~ {dmax}ï¼‰")
            
            conn.close()
            
        except Exception as e:
            logger.error(f"é©—è­‰åƒ¹æ ¼æ•¸æ“šå¤±æ•—: {e}")

def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å°è‚¡æ­·å²åƒ¹æ ¼æ•¸æ“šç²å–")
    parser.add_argument("--max-stocks", type=int, help="æœ€å¤§è™•ç†è‚¡ç¥¨æ•¸é‡ (æ¸¬è©¦ç”¨)")
    parser.add_argument("--bars", type=int, default=60, help="ç›®æ¨™Kç·šæ ¹æ•¸ (é è¨­60æ ¹)")
    parser.add_argument("--db-path", default="data/cleaned/taiwan_stocks_cleaned.db", help="è³‡æ–™åº«è·¯å¾‘")
    
    args = parser.parse_args()
    
    pipeline = TaiwanStockPriceDataPipeline(args.db_path)
    pipeline.run_price_data_pipeline(args.max_stocks, args.bars)

if __name__ == "__main__":
    main()